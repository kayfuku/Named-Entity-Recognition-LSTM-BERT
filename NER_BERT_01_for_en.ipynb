{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_BERT_01_for_en.ipynb","provenance":[{"file_id":"1ZL-ozV-8KnzTqbf8PROZHS8SBr7uRPnO","timestamp":1586231888304},{"file_id":"1lUqIXqGMWPwb96OvZZHChkvgctZjpvQP","timestamp":1585889381117},{"file_id":"1eD2syw6wZUJbsr_sbjIVyvkmTaVf8k_k","timestamp":1585886025372},{"file_id":"1HMbsNTR2ZGQ9K8D2wZINQKfefLAwDyFh","timestamp":1585796543095},{"file_id":"17QBibb081pFtgTmSnIpFV7Ixq3FzPiLH","timestamp":1579656913093}],"collapsed_sections":["GR64HT-ow1A8","UzgFplvGw65A"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"do805zSlwTue","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"BpwshrgrwBsn","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Azml1NhpYZk","colab_type":"code","outputId":"9f585cf9-39f7-4b3d-c485-36c105b202fd","executionInfo":{"status":"ok","timestamp":1586910846214,"user_tz":420,"elapsed":2767,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# Type of GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wed Apr 15 00:34:04 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1eh85yjpvda","colab_type":"code","outputId":"63d01713-56fa-4dca-b075-d717e129d18b","executionInfo":{"status":"ok","timestamp":1586910846216,"user_tz":420,"elapsed":2761,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Memory\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3pSBNlZwGFD","colab_type":"code","outputId":"bdbccaf9-ac19-4e36-d90b-27ade4a82cd8","executionInfo":{"status":"ok","timestamp":1586910854960,"user_tz":420,"elapsed":11498,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":928}},"source":["!pip install seqeval transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.2)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 13.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 37.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 46.7MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=8cb5950d303920c0d84627cff0323cf2f3f0805d3085c7af8b56af47f50273b1\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=6a8fa0886ceeae65696b91974bee85b59e74002b3bc6a435f88d878132e0032b\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 seqeval-0.0.12 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AY7mEyiqwXw7","colab_type":"code","outputId":"528560fc-0933-402e-9dbf-713d3d199b4a","executionInfo":{"status":"ok","timestamp":1586910858024,"user_tz":420,"elapsed":14555,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["!mkdir data\n","!mkdir models\n","# !wget https://raw.githubusercontent.com/Hironsan/IOB2Corpus/master/ja.wikipedia.conll -P data/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-15 00:34:17--  https://raw.githubusercontent.com/Hironsan/IOB2Corpus/master/ja.wikipedia.conll\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1297592 (1.2M) [text/plain]\n","Saving to: ‘data/ja.wikipedia.conll’\n","\n","\rja.wikipedia.conll    0%[                    ]       0  --.-KB/s               \rja.wikipedia.conll  100%[===================>]   1.24M  --.-KB/s    in 0.06s   \n","\n","2020-04-15 00:34:17 (20.0 MB/s) - ‘data/ja.wikipedia.conll’ saved [1297592/1297592]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v7GSLaHewmV2","colab_type":"text"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"MVaLQ4f4wkNh","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utilities.\n","\"\"\"\n","from seqeval.metrics import classification_report\n","\n","\n","def load_dataset(filename, encoding='utf-8'):\n","    \"\"\"Loads data and label from a file.\n","    Args:\n","        filename (str): path to the file.\n","        encoding (str): file encoding format.\n","        The file format is tab-separated values.\n","        A blank line is required at the end of a sentence.\n","        For example:\n","        ```\n","        EU\tB-ORG\n","        rejects\tO\n","        German\tB-MISC\n","        call\tO\n","        to\tO\n","        boycott\tO\n","        British\tB-MISC\n","        lamb\tO\n","        .\tO\n","        Peter\tB-PER\n","        Blackburn\tI-PER\n","        ...\n","        ```\n","    Returns:\n","        tuple(numpy array, numpy array): data and labels.\n","    Example:\n","        >>> filename = 'conll2003/en/ner/train.txt'\n","        >>> data, labels = load_data_and_labels(filename)\n","    \"\"\"\n","    sentences, labels = [], []\n","    words, tags = [], []\n","    with open(filename, encoding=encoding) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line:\n","                word, tag = line.split('\\t')\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                sentences.append(words)\n","                labels.append(tags)\n","                words, tags = [], []\n","        if words:\n","            sentences.append(words)\n","            labels.append(tags)\n","\n","    return sentences, labels\n","\n","# For BERT\n","def evaluate(model, labels_vocab, features, label_ids_true):\n","    # Predict. \n","    label_ids_pred = model.predict(features)\n","    print('label_ids_pred after predict:\\n', label_ids_pred)\n","    label_ids_pred = np.argmax(label_ids_pred[0], axis=-1) # label_ids_pred[0] <= typo corrected! \n","    print('label_ids_pred after argmax:\\n', label_ids_pred)\n","    print('label_ids_true:\\n', label_ids_true)\n","\n","    y_pred = [[] for _ in range(label_ids_pred.shape[0])]\n","    y_true = [[] for _ in range(label_ids_pred.shape[0])]\n","    for i in range(label_ids_pred.shape[0]):\n","        for j in range(label_ids_pred.shape[1]):\n","            if label_ids_true[i][j] == 0:\n","                continue\n","            y_pred[i].append(label_ids_pred[i][j])\n","            y_true[i].append(label_ids_true[i][j])\n","\n","    y_pred = labels_vocab.decode(y_pred)\n","    y_true = labels_vocab.decode(y_true)\n","    print(classification_report(y_true, y_pred, digits=4))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIl_MoqCwqAu","colab_type":"text"},"source":["# preprocessing.py"]},{"cell_type":"code","metadata":{"id":"wBb1bBB4wpUR","colab_type":"code","colab":{}},"source":["import re\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class Vocab:\n","\n","    def __init__(self, num_words=None, lower=True, oov_token=None):\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","            num_words=num_words, # max size of vocabulary\n","            oov_token=oov_token,\n","            filters='',\n","            lower=lower,\n","            split='\\t'\n","        )\n","\n","    def fit(self, sequences):\n","        texts = self._texts(sequences)\n","        # Create vocabulary. \n","        self.tokenizer.fit_on_texts(texts)\n","        return self\n","\n","    def encode(self, sequences):\n","        \"\"\" Convert words to ids \"\"\"\n","        texts = self._texts(sequences)\n","        # print('texts in encode():', texts[:5]) # list of strings (one string per sentence)\n","        return self.tokenizer.texts_to_sequences(texts) # For one string, change string to list of ids. \n","\n","    def decode(self, sequences):\n","        # print('sequences in decode:\\n', sequences)\n","        texts = self.tokenizer.sequences_to_texts(sequences)\n","        return [text.split(' ') for text in texts]\n","\n","    def _texts(self, sequences):\n","        return ['\\t'.join(words) for words in sequences]\n","\n","    def get_index(self, word):\n","        return self.tokenizer.word_index.get(word)\n","\n","    @property\n","    def size(self):\n","        \"\"\"Return vocabulary size.\"\"\"\n","        return len(self.tokenizer.word_index) + 1\n","\n","    def save(self, file_path):\n","        with open(file_path, 'w') as f:\n","            config = self.tokenizer.to_json()\n","            f.write(config)\n","\n","    @classmethod\n","    def load(cls, file_path):\n","        with open(file_path) as f:\n","            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n","            vocab = cls()\n","            vocab.tokenizer = tokenizer\n","        return vocab\n","\n","\n","def normalize_number(text, reduce=True):\n","    \"\"\" Replace numbers with 0. \"\"\"\n","    if reduce:\n","        normalized_text = re.sub(r'\\d+', '0', text)\n","    else:\n","        # Keep the length same. \n","        normalized_text = re.sub(r'\\d', '0', text)\n","    return normalized_text\n","\n","\n","def preprocess_dataset(sequences):\n","    sequences = [[normalize_number(w) for w in words] for words in sequences]\n","    return sequences\n","\n","\n","def create_dataset(sequences, vocab):\n","    # print('before encode:', sequences[:5])\n","    sequences = vocab.encode(sequences)\n","    # print('after encode:', sequences[:5])\n","    # Padding\n","    sequences = pad_sequences(sequences, padding='post')\n","    return sequences\n","\n","\n","# Create inputs for BERT. \n","def convert_examples_to_features(x, # x_train\n","                                 y, # y_train\n","                                 vocab, # Vocabulary of lebels\n","                                 max_seq_length,\n","                                 tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","        'label_ids': []\n","    }\n","    for words, labels in zip(x, y):\n","        # print('words:', words) # sentence\n","        # print('labels:', labels) # labels in the sentence\n","\n","        # For each sentence \n","        tokens = [tokenizer.cls_token] # [CLS]\n","        # print('tokens:', tokens)\n","\n","        label_ids = [pad_token]\n","        for word, label in zip(words, labels):\n","            # For each word  \n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            label_id = vocab.get_index(label)\n","            label_ids.extend([label_id] + [pad_token] * (len(word_tokens) - 1))\n","\n","        tokens += [tokenizer.sep_token] # [SEP]\n","\n","        # print('tokens before convert_tokens_to_ids:\\n', tokens) \n","        # ['[CLS]', 'キ', '##ケ', '##ロ', 'は', '、', 'カエサル', 'と', 'は', ..., 'こと', 'と', 'なっ', 'た', '。', '[SEP]']\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        # print('input_ids:\\n', input_ids)\n","        # [2, 185, 28719, 28505, 9, 6, 18936, 13, ... ]\n","\n","        attention_mask = [1] * len(input_ids)\n","        # print('attention_mask:\\n', attention_mask)\n","        # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]\n","\n","        token_type_ids = [pad_token] * max_seq_length\n","        # print('token_type_ids:\\n', token_type_ids)\n","        # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]\n","\n","        # print('label_ids:\\n', label_ids)\n","        # [0, 7, 0, 0, 1, 1, 7, 1, 1, 1, 1, 16, 15, 1, 1, ... ]\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","        features['label_ids'].append(label_ids)\n","\n","    # Padding\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    y = features['label_ids']\n","    return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhcqeEKwwDg","colab_type":"text"},"source":["# models.py"]},{"cell_type":"code","metadata":{"id":"x2BAMgPuwuBU","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional\n","from transformers import TFBertForTokenClassification, BertConfig\n","\n","\n","class UnidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim,\n","                         return_sequences=True, # Point! True: Sequence Labeling\n","                         name='lstm')\n","        # output_dim: label_vocab.size()\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.lstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class BidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        lstm = LSTM(hid_dim,\n","                    return_sequences=True,\n","                    name='lstm')\n","        # Wrap the LSTM with Bidirectional. \n","        self.bilstm = Bidirectional(lstm, name='bilstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        bilstm = self.bilstm(embedding)\n","        y = self.fc(bilstm)\n","        return Model(inputs=x, outputs=y)\n","\n","# For BERT\n","def build_model(pretrained_model_name_or_path, num_labels):\n","    # BertConfig holds configuration for BERT. \n","    # Read configuration from pre-trained model. \n","    config = BertConfig.from_pretrained(\n","        pretrained_model_name_or_path,\n","        num_labels=num_labels\n","    )\n","    # BERT for sequence labelling\n","    model = TFBertForTokenClassification.from_pretrained(\n","        pretrained_model_name_or_path,\n","        config=config\n","    )\n","    # Add a Dense layer with softmax to the last layer.  \n","    model.layers[-1].activation = tf.keras.activations.softmax\n","    \n","    return model\n","\n","\n","def loss_func(num_labels):\n","    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def loss(y_true, y_pred):\n","        input_mask = tf.not_equal(y_true, 0)\n","        logits = tf.reshape(y_pred, (-1, num_labels))\n","        active_loss = tf.reshape(input_mask, (-1,))\n","        # Remove paddings. \n","        active_logits = tf.boolean_mask(logits, active_loss)\n","        train_labels = tf.reshape(y_true, (-1,))\n","        # Remove paddings. \n","        active_labels = tf.boolean_mask(train_labels, active_loss)\n","        cross_entropy = loss_fct(active_labels, active_logits)\n","        return cross_entropy\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR64HT-ow1A8","colab_type":"text"},"source":["# inference.py"]},{"cell_type":"code","metadata":{"id":"9AlqMrRUw0bK","colab_type":"code","colab":{}},"source":["\"\"\"\n","Inference API.\n","\"\"\"\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class InferenceAPI:\n","    \"\"\"A model API that generates output sequence.\n","\n","    Attributes:\n","        model: Model\n","        words_vocab: vocabulary of words\n","        labels_vocab: vocabulary of labels\n","    \"\"\"\n","\n","    def __init__(self, model, words_vocab, labels_vocab):\n","        self.model = model\n","        self.words_vocab = words_vocab\n","        self.labels_vocab = labels_vocab\n","\n","    def predict_from_sequences(self, sequences):\n","        lengths = map(len, sequences)\n","        # Convert words to ids. \n","        sequences = self.words_vocab.encode(sequences)\n","        sequences = pad_sequences(sequences, padding='post')\n","        # Predict. \n","        y_pred = self.model.predict(sequences)\n","        print('y_pred after predict:', y_pred[:5])\n","        y_pred = np.argmax(y_pred, axis=-1)\n","        print('y_pred after argmax:', y_pred[:5])\n","        # Convert ids of labels to labels.\n","        y_pred = self.labels_vocab.decode(y_pred)  \n","        print('y_pred after decode:', y_pred[:5])\n","        # ??\n","        y_pred = [y[:l] for y, l in zip(y_pred, lengths)]\n","        print('y_pred after the last line:', y_pred[:5])\n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzgFplvGw65A","colab_type":"text"},"source":["# train.py"]},{"cell_type":"code","metadata":{"id":"4V1kaXAjw36S","colab_type":"code","colab":{}},"source":["# from sklearn.model_selection import train_test_split\n","# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","# from tensorflow.keras.models import load_model\n","# from seqeval.metrics import classification_report\n","# from google.colab import files \n","# import io \n","\n","\n","# def main():\n","#     # Set hyper-parameters.\n","#     batch_size = 32\n","#     epochs = 100\n","#     # model_path = 'models/model_{}.h5'\n","#     model_path = 'models/bidirectional_model_{}.h5'\n","#     num_words = 15000 # Max size of vocabulary\n","#     # num_words = 150000 # Max size of vocabulary\n","\n","#     # Data loading.\n","#     # x: sentences, y: labels\n","#     # x, y = load_dataset('./data/ja.wikipedia.conll')\n","#     x, y = load_dataset('./test_empty_line_inserted.tsv')    \n","    \n","#     # Upload file from local. \n","# #     uploaded = files.upload() \n","# # 　　　　　　　ner_labeled_data = pd.read_csv(io.BytesIO(uploaded['test_empty_line_inserted.tsv'])) \n","\n","\n","#     # Pre-processing.\n","#     x = preprocess_dataset(x) # Normalize numbers. \n","#     # Train test split\n","#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","#     # Create vocabularies. \n","#     words_vocab = Vocab(num_words=num_words, oov_token='<UNK>').fit(x_train)\n","#     labels_vocab = Vocab(lower=False).fit(y_train)\n","    \n","#     # Convert words to ids. \n","#     x_train = create_dataset(x_train, words_vocab)\n","#     print('train words:', x_train[:5])\n","#     y_train = create_dataset(y_train, labels_vocab)\n","#     print('train labels:', y_train[:5])\n","\n","#     # Build models.\n","#     models = [\n","#         # UnidirectionalModel(num_words, labels_vocab.size).build(),\n","#         BidirectionalModel(num_words, labels_vocab.size).build(),\n","#     ]\n","\n","#     for i, model in enumerate(models):\n","#         model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","#         # Preparing callbacks.\n","#         callbacks = [\n","#             EarlyStopping(patience=3),\n","#             ModelCheckpoint(model_path.format(i), save_best_only=True)\n","#         ]\n","\n","#         # Train the model.\n","#         model.fit(x=x_train,\n","#                   y=y_train,\n","#                   batch_size=batch_size,\n","#                   epochs=epochs,\n","#                   validation_split=0.1,\n","#                   callbacks=callbacks,\n","#                   shuffle=True)\n","\n","#         # Inference.\n","#         model = load_model(model_path.format(i))\n","#         api = InferenceAPI(model, words_vocab, labels_vocab)\n","#         y_pred = api.predict_from_sequences(x_test)\n","#         print(classification_report(y_test, y_pred, digits=4))\n","\n","\n","# if __name__ == '__main__':\n","#     main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAO2aikw99QU","colab_type":"text"},"source":["# train_bert.py\n","\n"]},{"cell_type":"code","metadata":{"id":"KsHyuz10-E2a","colab_type":"code","outputId":"b5b020ba-6722-46a4-edde-8354600a4be6","executionInfo":{"status":"ok","timestamp":1586932141186,"user_tz":420,"elapsed":145025,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from transformers import BertTokenizer, DistilBertTokenizer\n","\n","# from models import build_model, loss_func\n","# from preprocessing import convert_examples_to_features, Vocab, preprocess_dataset\n","# from utils import load_dataset, evaluate\n","\n","\n","def main():\n","    # Set hyper-parameters.\n","    batch_size = 32\n","    epochs = 100\n","    # epochs = 20 # For debugging\n","    model_path = 'models/'\n","    # Pre-trained model trained on the DistilBERT model \n","    # distilled from the BERT model bert-base-cased\n","    # pretrained_model_name_or_path = 'distilbert-base-cased'\n","    # Pre-trained model trained on cased English text.\n","    pretrained_model_name_or_path = 'bert-base-cased'\n","    maxlen = 250\n","\n","    # Data loading.\n","    x, y = load_dataset('./test_empty_line_inserted.tsv')    \n","\n","    # Tokenizer from BERT\n","    # tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path, do_word_tokenize=False)\n","    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path, do_word_tokenize=False)\n","\n","    # Pre-processing.\n","    x = preprocess_dataset(x) # Normalize numbers.\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","    labels_vocab = Vocab(lower=False).fit(y_train)\n","\n","    features_train, labels_train = convert_examples_to_features(\n","        x_train,\n","        y_train,\n","        # x_train[:5], # For debugging\n","        # y_train[:5],\n","        labels_vocab,\n","        max_seq_length=maxlen,\n","        tokenizer=tokenizer\n","    )\n","    \n","    features_test, labels_test = convert_examples_to_features(\n","        x_test,\n","        y_test,\n","        # x_test[:5], # For debugging\n","        # y_test[:5], \n","        labels_vocab,\n","        max_seq_length=maxlen,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Build model.\n","    model = build_model(pretrained_model_name_or_path, labels_vocab.size)\n","    # # Load trained model. \n","    # model = build_model(model_path, labels_vocab.size)\n","    model.compile(optimizer='sgd', loss=loss_func(labels_vocab.size))\n","\n","    # Preparing callbacks.\n","    callbacks = [\n","        EarlyStopping(patience=3),\n","    ]\n","\n","    # Train the model.\n","    model.fit(x=features_train,\n","              y=labels_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_split=0.1,\n","              callbacks=callbacks,\n","              shuffle=True)\n","    model.save_pretrained(model_path)\n","\n","    # Evaluation.\n","    evaluate(model, labels_vocab, features_test, labels_test)\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["label_ids_pred after predict:\n"," (array([[[3.84216546e-05, 9.91067827e-01, 1.51462213e-03, ...,\n","         3.36913508e-05, 1.93526575e-04, 8.07656397e-05],\n","        [7.76241052e-07, 9.99349892e-01, 2.89793446e-04, ...,\n","         9.14278814e-07, 5.11719009e-06, 1.04136654e-06],\n","        [8.14651648e-05, 2.78127816e-04, 9.35651302e-01, ...,\n","         4.98961126e-05, 3.91494192e-04, 8.05361196e-05],\n","        ...,\n","        [3.85773092e-06, 9.96182024e-01, 9.98642761e-04, ...,\n","         8.40593839e-06, 2.24059186e-05, 6.60109345e-06],\n","        [3.31167007e-06, 9.96991158e-01, 7.23084202e-04, ...,\n","         6.84519409e-06, 1.96578640e-05, 5.55329143e-06],\n","        [2.03866671e-06, 9.98490214e-01, 3.33804317e-04, ...,\n","         3.63085201e-06, 1.14772984e-05, 3.38397103e-06]],\n","\n","       [[1.16683383e-04, 9.75640655e-01, 2.81129475e-03, ...,\n","         8.11719001e-05, 5.65593888e-04, 2.46033160e-04],\n","        [2.44198645e-07, 9.99945521e-01, 3.63537424e-06, ...,\n","         1.53141258e-07, 9.79434503e-07, 3.52907279e-07],\n","        [2.47786801e-07, 9.99950409e-01, 3.29034719e-06, ...,\n","         1.84396072e-07, 8.26475230e-07, 3.55523582e-07],\n","        ...,\n","        [6.44988613e-05, 8.18590224e-01, 1.90281961e-02, ...,\n","         1.42567776e-04, 2.75673461e-04, 7.48549501e-05],\n","        [1.10515621e-05, 9.82569218e-01, 2.12261127e-03, ...,\n","         2.18928217e-05, 5.15475658e-05, 1.49471880e-05],\n","        [2.21769787e-05, 9.73290861e-01, 4.02118172e-03, ...,\n","         3.89608394e-05, 1.08581531e-04, 3.04768619e-05]],\n","\n","       [[1.29730921e-04, 9.74442244e-01, 2.79427157e-03, ...,\n","         1.09697903e-04, 6.53348165e-04, 2.83958623e-04],\n","        [6.59214095e-07, 9.99575078e-01, 2.81462162e-05, ...,\n","         5.25233531e-07, 2.67625637e-06, 7.52962421e-07],\n","        [6.76832769e-07, 9.99790728e-01, 1.23421096e-05, ...,\n","         9.63679554e-07, 1.41563646e-06, 7.44118665e-07],\n","        ...,\n","        [1.40873672e-04, 6.81118011e-01, 4.29728292e-02, ...,\n","         2.62247631e-04, 5.86699694e-04, 1.55453483e-04],\n","        [1.29743465e-04, 7.44684041e-01, 3.57051753e-02, ...,\n","         2.40643596e-04, 6.50766829e-04, 1.65119258e-04],\n","        [8.57527411e-05, 8.71692300e-01, 1.80812199e-02, ...,\n","         1.51685774e-04, 3.29752016e-04, 9.21858955e-05]],\n","\n","       ...,\n","\n","       [[5.34178616e-05, 9.86309052e-01, 2.08477792e-03, ...,\n","         5.11905091e-05, 2.48332915e-04, 1.27007545e-04],\n","        [9.24451251e-06, 1.34183050e-04, 6.46154731e-05, ...,\n","         1.11856956e-04, 5.87158138e-06, 8.26408541e-06],\n","        [2.28038207e-05, 9.13184322e-03, 7.67676902e-05, ...,\n","         5.34082465e-05, 4.46712256e-05, 3.36902995e-05],\n","        ...,\n","        [5.51346375e-06, 9.92637157e-01, 1.40634773e-03, ...,\n","         1.65994534e-05, 2.50666253e-05, 1.06301459e-05],\n","        [5.61132838e-05, 8.80695522e-01, 3.95224094e-02, ...,\n","         1.36268514e-04, 2.13385865e-04, 6.74507319e-05],\n","        [5.46145493e-05, 9.75389481e-01, 5.73109929e-03, ...,\n","         1.07080210e-04, 2.11515362e-04, 5.97454818e-05]],\n","\n","       [[7.21467004e-05, 9.86522734e-01, 1.93014310e-03, ...,\n","         5.28803903e-05, 3.59528873e-04, 1.35602895e-04],\n","        [2.76334913e-06, 9.53787625e-01, 3.07769544e-04, ...,\n","         2.21387154e-06, 1.12728139e-05, 3.76085814e-06],\n","        [1.23860600e-05, 4.31919325e-04, 4.18335106e-03, ...,\n","         3.75172349e-05, 3.83834558e-05, 2.29468842e-05],\n","        ...,\n","        [1.31003500e-04, 6.47523766e-03, 1.04567653e-03, ...,\n","         1.27219231e-04, 2.99558742e-04, 2.78536725e-04],\n","        [5.69281518e-04, 3.01005989e-02, 4.24885303e-02, ...,\n","         4.90589184e-04, 2.04185280e-03, 6.31881412e-04],\n","        [2.70223740e-04, 3.01907491e-03, 5.87955175e-04, ...,\n","         2.27772456e-04, 2.45679490e-04, 4.77422174e-04]],\n","\n","       [[1.36043454e-04, 9.66960430e-01, 5.79888839e-03, ...,\n","         1.09162720e-04, 6.54438743e-04, 3.16833728e-04],\n","        [8.92694516e-06, 2.87034236e-05, 1.94441222e-04, ...,\n","         8.74163234e-05, 6.61549484e-06, 7.40443238e-06],\n","        [3.57448334e-06, 9.95302796e-01, 1.38189891e-04, ...,\n","         1.05359004e-05, 7.37534083e-06, 7.12668361e-06],\n","        ...,\n","        [3.17238475e-04, 4.78489429e-01, 9.06890035e-02, ...,\n","         7.47819082e-04, 9.96238086e-04, 3.70371359e-04],\n","        [1.90400504e-04, 6.06885135e-01, 6.42772019e-02, ...,\n","         4.68533748e-04, 8.70975782e-04, 2.27214870e-04],\n","        [3.08294693e-04, 3.71929914e-01, 1.30148798e-01, ...,\n","         7.06868886e-04, 9.94500471e-04, 3.69185262e-04]]], dtype=float32),)\n","label_ids_pred after argmax:\n"," [[ 1  1  2 ...  1  1  1]\n"," [ 1  1  1 ...  1  1  1]\n"," [ 1  1  1 ...  1  1  1]\n"," ...\n"," [ 1  8  6 ...  1  1  1]\n"," [ 1  1  4 ... 10  3 10]\n"," [ 1  8  1 ...  1  1  1]]\n","label_ids_true:\n"," [[0 1 3 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]\n"," ...\n"," [0 8 6 ... 0 0 0]\n"," [0 1 4 ... 0 0 0]\n"," [0 8 1 ... 0 0 0]]\n","           precision    recall  f1-score   support\n","\n","      geo     0.8439    0.8847    0.8638      7664\n","      org     0.7002    0.6619    0.6805      3913\n","      tim     0.8039    0.8614    0.8317      4049\n","      per     0.7155    0.8014    0.7560      3389\n","      gpe     0.8297    0.9528    0.8870      3175\n","      art     0.1316    0.0581    0.0806        86\n","      eve     0.1935    0.2000    0.1967        60\n","      nat     0.3333    0.1400    0.1972        50\n","\n","micro avg     0.7879    0.8319    0.8093     22386\n","macro avg     0.7845    0.8319    0.8066     22386\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vyfXQ7zG52Yh","colab_type":"code","outputId":"8e0bcf65-5e64-4581-9c7c-e882230307aa","executionInfo":{"status":"ok","timestamp":1586931911269,"user_tz":420,"elapsed":120598,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":472}},"source":["# # Download files.\n","\n","# from google.colab import files\n","# files.download('models/tf_model.h5')\n","# files.download('models/config.json')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Exception happened during processing of request from ('::ffff:127.0.0.1', 59920, 0, 0)\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n","    self.process_request(request, client_address)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n","    self.finish_request(request, client_address)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n","    self.RequestHandlerClass(request, client_address, self)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n","    self.handle()\n","  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n","    self.handle_one_request()\n","  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n","    method()\n","  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n","    self.copyfile(f, self.wfile)\n","  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n","    shutil.copyfileobj(source, outputfile)\n","  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n","    fdst.write(buf)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n","    self._sock.sendall(b)\n","ConnectionResetError: [Errno 104] Connection reset by peer\n","----------------------------------------\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pesJj70ZjoBE","colab_type":"code","outputId":"d2010229-1183-4d1b-fb69-53eed549a764","executionInfo":{"status":"ok","timestamp":1586932670867,"user_tz":420,"elapsed":22072,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["# Mount my Google Drive. \n","from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aV3wa4vUjosX","colab_type":"code","colab":{}},"source":["# Stash the trained models to Google Drive. \n","!cp -r ./models/ drive/'My Drive'/bert/bert/trained_models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KC5t6zDEkTvH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}