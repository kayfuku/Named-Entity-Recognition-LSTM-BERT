{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_LSTM_01_for_en.ipynb","provenance":[{"file_id":"1lUqIXqGMWPwb96OvZZHChkvgctZjpvQP","timestamp":1585889381117},{"file_id":"1eD2syw6wZUJbsr_sbjIVyvkmTaVf8k_k","timestamp":1585886025372},{"file_id":"1HMbsNTR2ZGQ9K8D2wZINQKfefLAwDyFh","timestamp":1585796543095},{"file_id":"17QBibb081pFtgTmSnIpFV7Ixq3FzPiLH","timestamp":1579656913093}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"do805zSlwTue","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"BpwshrgrwBsn","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3pSBNlZwGFD","colab_type":"code","outputId":"77bf0edc-4649-4ca8-e1ee-38c57e8f1523","executionInfo":{"status":"ok","timestamp":1587702180303,"user_tz":420,"elapsed":4342,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":563}},"source":["!pip install seqeval transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.2)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AY7mEyiqwXw7","colab_type":"code","outputId":"3863c522-6741-462e-d0cc-c762036e34cc","executionInfo":{"status":"ok","timestamp":1587702227440,"user_tz":420,"elapsed":51325,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["!mkdir data\n","!mkdir models\n","# Pre-trained word embeddings for English language\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz -P data/ "],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n","mkdir: cannot create directory ‘models’: File exists\n","--2020-04-24 04:23:02--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1325960915 (1.2G) [binary/octet-stream]\n","Saving to: ‘data/cc.en.300.vec.gz.3’\n","\n","cc.en.300.vec.gz.3  100%[===================>]   1.23G  29.8MB/s    in 44s     \n","\n","2020-04-24 04:23:46 (29.0 MB/s) - ‘data/cc.en.300.vec.gz.3’ saved [1325960915/1325960915]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v7GSLaHewmV2","colab_type":"text"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"MVaLQ4f4wkNh","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utilities.\n","\"\"\"\n","import string\n","import gensim\n","import numpy as np\n","import pandas as pd\n","\n","def load_dataset(filename, encoding='utf-8'):\n","    \"\"\"Loads data and label from a file.\n","    Args:\n","        filename (str): path to the file.\n","        encoding (str): file encoding format.\n","        The file format is tab-separated values.\n","        A blank line is required at the end of a sentence.\n","        For example:\n","        ```\n","        EU\tB-ORG\n","        rejects\tO\n","        German\tB-MISC\n","        call\tO\n","        to\tO\n","        boycott\tO\n","        British\tB-MISC\n","        lamb\tO\n","        .\tO\n","        Peter\tB-PER\n","        Blackburn\tI-PER\n","        ...\n","        ```\n","    Returns:\n","        tuple(numpy array, numpy array): data and labels.\n","    Example:\n","        >>> filename = 'conll2003/en/ner/train.txt'\n","        >>> data, labels = load_data_and_labels(filename)\n","    \"\"\"\n","    sentences, labels = [], []\n","    words, tags = [], []\n","    with open(filename, encoding=encoding) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line:\n","                word, tag = line.split('\\t')\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                sentences.append(words)\n","                labels.append(tags)\n","                words, tags = [], []\n","        if words:\n","            sentences.append(words)\n","            labels.append(tags)\n","\n","    return sentences, labels\n","\n","def filter_embeddings(embeddings, vocab, num_words, dim=300):\n","    \"\"\"Filter word vectors.\n","\n","    Args:\n","        embeddings: a dictionary like object.\n","        vocab: word-index lookup table.\n","        num_words: the number of words.\n","        dim: dimension.\n","\n","    Returns:\n","        numpy array: an array of word embeddings.\n","    \"\"\"\n","    _embeddings = np.zeros((num_words, dim))\n","    for word in vocab:\n","        if word in embeddings:\n","            word_id = vocab[word]\n","            # Get the word embedding of word whose id is less than num_words. \n","            # What is this? \n","            if word_id >= num_words:\n","                continue\n","            _embeddings[word_id] = embeddings[word]\n","\n","    return _embeddings\n","\n","\n","def load_fasttext(filepath, binary=False):\n","    \"\"\"Loads fastText vectors.\n","\n","    Args:\n","        filepath (str): a path to a fastText file.\n","\n","    Return:\n","        model: KeyedVectors\n","    \"\"\"\n","    model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=binary)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIl_MoqCwqAu","colab_type":"text"},"source":["# preprocessing.py"]},{"cell_type":"code","metadata":{"id":"wBb1bBB4wpUR","colab_type":"code","colab":{}},"source":["import re\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class Vocab:\n","\n","    def __init__(self, num_words=None, lower=True, oov_token=None):\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","            num_words=num_words, # max size of vocabulary\n","            oov_token=oov_token,\n","            filters='',\n","            lower=lower,\n","            split='\\t'\n","        )\n","\n","    def fit(self, sequences):\n","        texts = self._texts(sequences)\n","        # Create vocabulary. \n","        self.tokenizer.fit_on_texts(texts)\n","        return self\n","\n","    def encode(self, sequences):\n","        \"\"\" Convert words to ids \"\"\"\n","        texts = self._texts(sequences)\n","        print('texts in encode():', texts[:5]) # list of strings (one string per sentence)\n","        return self.tokenizer.texts_to_sequences(texts) # For one string, change string to list of ids. \n","\n","    def decode(self, sequences):\n","        texts = self.tokenizer.sequences_to_texts(sequences)\n","        return [text.split(' ') for text in texts]\n","\n","    def _texts(self, sequences):\n","        return ['\\t'.join(words) for words in sequences]\n","\n","    def get_index(self, word):\n","        return self.tokenizer.word_index.get(word)\n","\n","    @property\n","    def size(self):\n","        \"\"\"Return vocabulary size.\"\"\"\n","        return len(self.tokenizer.word_index) + 1\n","\n","    def save(self, file_path):\n","        with open(file_path, 'w') as f:\n","            config = self.tokenizer.to_json()\n","            f.write(config)\n","\n","    @classmethod\n","    def load(cls, file_path):\n","        with open(file_path) as f:\n","            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n","            vocab = cls()\n","            vocab.tokenizer = tokenizer\n","        return vocab\n","\n","\n","def normalize_number(text, reduce=True):\n","    \"\"\" Replace numbers with 0. \"\"\"\n","    if reduce:\n","        normalized_text = re.sub(r'\\d+', '0', text)\n","    else:\n","        # Keep the length same. \n","        normalized_text = re.sub(r'\\d', '0', text)\n","    return normalized_text\n","\n","\n","def preprocess_dataset(sequences):\n","    sequences = [[normalize_number(w) for w in words] for words in sequences]\n","    return sequences\n","\n","\n","def create_dataset(sequences, vocab):\n","    print('before encode:', sequences[:5])\n","    sequences = vocab.encode(sequences)\n","    print('after encode:', sequences[:5])\n","    # Padding\n","    sequences = pad_sequences(sequences, padding='post')\n","    return sequences"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhcqeEKwwDg","colab_type":"text"},"source":["# models.py"]},{"cell_type":"code","metadata":{"id":"x2BAMgPuwuBU","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional\n","\n","\n","class UnidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim,\n","                         return_sequences=True, # Point! True: Sequence Labeling\n","                         name='lstm')\n","        # output_dim: label_vocab.size()\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.lstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class BidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        lstm = LSTM(hid_dim,\n","                    return_sequences=True,\n","                    name='lstm')\n","        # Wrap the LSTM with Bidirectional. \n","        self.bilstm = Bidirectional(lstm, name='bilstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        bilstm = self.bilstm(embedding)\n","        y = self.fc(bilstm)\n","        return Model(inputs=x, outputs=y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR64HT-ow1A8","colab_type":"text"},"source":["# inference.py"]},{"cell_type":"code","metadata":{"id":"9AlqMrRUw0bK","colab_type":"code","colab":{}},"source":["\"\"\"\n","Inference API.\n","\"\"\"\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class InferenceAPI:\n","    \"\"\"A model API that generates output sequence.\n","\n","    Attributes:\n","        model: Model\n","        words_vocab: vocabulary of words\n","        labels_vocab: vocabulary of labels\n","    \"\"\"\n","\n","    def __init__(self, model, words_vocab, labels_vocab):\n","        self.model = model\n","        self.words_vocab = words_vocab\n","        self.labels_vocab = labels_vocab\n","\n","    def predict_from_sequences(self, sequences):\n","        lengths = map(len, sequences)\n","        # Convert words to ids. \n","        sequences = self.words_vocab.encode(sequences)\n","        sequences = pad_sequences(sequences, padding='post')\n","        # Predict. \n","        y_pred = self.model.predict(sequences)\n","        print('y_pred after predict:', y_pred[:5])\n","        y_pred = np.argmax(y_pred, axis=-1)\n","        print('y_pred after argmax:', y_pred[:5])\n","        # Convert ids of labels to labels.\n","        y_pred = self.labels_vocab.decode(y_pred)  \n","        print('y_pred after decode:', y_pred[:5])\n","        # ??\n","        y_pred = [y[:l] for y, l in zip(y_pred, lengths)]\n","        print('y_pred after the last line:', y_pred[:5])\n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzgFplvGw65A","colab_type":"text"},"source":["# train.py"]},{"cell_type":"code","metadata":{"id":"4V1kaXAjw36S","colab_type":"code","outputId":"187ca72d-4705-46ad-c615-e2923c4a9bf0","executionInfo":{"status":"ok","timestamp":1587703750658,"user_tz":420,"elapsed":1573295,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","from seqeval.metrics import classification_report\n","from google.colab import files \n","import io \n","\n","\n","def main():\n","    # Set hyper-parameters.\n","    batch_size = 32\n","    epochs = 100\n","    model_path = 'models/model_{}.h5'\n","    # model_path = 'models/bidirectional_model_{}.h5'\n","    # num_words = 15000 # Max size of vocabulary\n","    num_words = 30522 # Max size of vocabulary\n","    # num_words = 150000 # Max size of vocabulary\n","\n","    # Load data. \n","    # x: sentences, y: labels\n","    x, y = load_dataset('./test_empty_line_inserted.tsv')    \n","    \n","    # Upload file from local. \n","#     uploaded = files.upload() \n","# 　　　　　　　ner_labeled_data = pd.read_csv(io.BytesIO(uploaded['test_empty_line_inserted.tsv'])) \n","\n","    # Pre-process data. \n","    x = preprocess_dataset(x) # Normalize numbers. \n","    # Split into train and test. \n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","    # Create vocabularies. \n","    words_vocab = Vocab(num_words=num_words, oov_token='<UNK>').fit(x_train)\n","    labels_vocab = Vocab(lower=False).fit(y_train)\n","    \n","    # Convert words to ids. \n","    x_train = create_dataset(x_train, words_vocab)\n","    print('train words:', x_train[:5])\n","    y_train = create_dataset(y_train, labels_vocab)\n","    print('train labels:', y_train[:5])\n","\n","    # Prepare word embedding.\n","    print('loading fastText...')\n","    wv = load_fasttext('data/cc.en.300.vec.gz')\n","    print('filtering embeddings...')\n","    wv = filter_embeddings(wv, words_vocab.tokenizer.word_index, num_words)\n","\n","    # Build models.\n","    models = [\n","        # LSTM\n","        # UnidirectionalModel(num_words, labels_vocab.size).build(),\n","        # UnidirectionalModel(num_words, labels_vocab.size, embeddings=wv).build(),\n","\n","        # Bi-LSTM\n","        # BidirectionalModel(num_words, labels_vocab.size).build(),\n","        BidirectionalModel(num_words, labels_vocab.size, embeddings=wv).build(),\n","    ]\n","\n","    for i, model in enumerate(models):\n","        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","        # Preparing callbacks.\n","        callbacks = [\n","            EarlyStopping(patience=3),\n","            ModelCheckpoint(model_path.format(i), save_best_only=True)\n","        ]\n","\n","        # Train the model.\n","        model.fit(x=x_train,\n","                  y=y_train,\n","                  batch_size=batch_size,\n","                  epochs=epochs,\n","                  validation_split=0.1,\n","                  callbacks=callbacks,\n","                  shuffle=True)\n","\n","        # Inference.\n","        model = load_model(model_path.format(i))\n","        api = InferenceAPI(model, words_vocab, labels_vocab)\n","        y_pred = api.predict_from_sequences(x_test)\n","        print(classification_report(y_test, y_pred, digits=4))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["before encode: [['The', '0-year-old', 'former', 'analyst', 'says', 'he', 'provided', 'information', 'to', 'an', 'official', 'at', 'the', 'Israeli', 'embassy', 'and', 'to', 'two', 'members', 'of', 'a', 'lobbying', 'group', 'called', 'the', 'American', 'Israel', 'Public', 'Affairs', 'Committee', '.'], ['But', 'he', 'said', 'he', 'will', 'not', 'accept', 'any', 'Pakistani', 'proposal', 'that', 'involves', 'redrawing', 'the', 'line', 'of', 'control', 'that', 'separates', 'Indian-', 'from', 'Pakistani-controlled', 'Kashmir', '.'], ['The', 'Swiss', 'star', 'was', 'upset', 'Wednesday', 'by', 'German', 'Tommy', 'Haas', 'in', 'the', 'opening', 'match', 'of', 'the', 'Kooyong', 'Classic', 'in', 'Melbourne', '.'], ['After', 'taking', 'office', 'in', '0', ',', 'the', 'SPENCER', 'government', 'adopted', 'an', 'ambitious', 'fiscal', 'reform', 'program', ',', 'and', 'was', 'successful', 'in', 'reducing', 'its', 'public', 'debt-to-GDP', 'ratio', 'from', '0', '%', 'to', 'about', '0', '%', 'in', '0', '.'], ['Israeli', 'soldiers', 'have', 'killed', 'two', 'Palestinians', 'in', 'the', 'southern', 'Gaza', 'Strip', '.']]\n","texts in encode(): ['The\\t0-year-old\\tformer\\tanalyst\\tsays\\the\\tprovided\\tinformation\\tto\\tan\\tofficial\\tat\\tthe\\tIsraeli\\tembassy\\tand\\tto\\ttwo\\tmembers\\tof\\ta\\tlobbying\\tgroup\\tcalled\\tthe\\tAmerican\\tIsrael\\tPublic\\tAffairs\\tCommittee\\t.', 'But\\the\\tsaid\\the\\twill\\tnot\\taccept\\tany\\tPakistani\\tproposal\\tthat\\tinvolves\\tredrawing\\tthe\\tline\\tof\\tcontrol\\tthat\\tseparates\\tIndian-\\tfrom\\tPakistani-controlled\\tKashmir\\t.', 'The\\tSwiss\\tstar\\twas\\tupset\\tWednesday\\tby\\tGerman\\tTommy\\tHaas\\tin\\tthe\\topening\\tmatch\\tof\\tthe\\tKooyong\\tClassic\\tin\\tMelbourne\\t.', 'After\\ttaking\\toffice\\tin\\t0\\t,\\tthe\\tSPENCER\\tgovernment\\tadopted\\tan\\tambitious\\tfiscal\\treform\\tprogram\\t,\\tand\\twas\\tsuccessful\\tin\\treducing\\tits\\tpublic\\tdebt-to-GDP\\tratio\\tfrom\\t0\\t%\\tto\\tabout\\t0\\t%\\tin\\t0\\t.', 'Israeli\\tsoldiers\\thave\\tkilled\\ttwo\\tPalestinians\\tin\\tthe\\tsouthern\\tGaza\\tStrip\\t.']\n","after encode: [[2, 350, 116, 7190, 22, 25, 1127, 390, 7, 26, 157, 21, 2, 124, 623, 9, 7, 42, 190, 6, 8, 7191, 77, 127, 2, 180, 125, 329, 1111, 596, 3], [48, 25, 19, 25, 35, 44, 1613, 243, 275, 998, 16, 3566, 16244, 2, 1476, 6, 353, 16, 16245, 16246, 24, 10131, 617, 3], [2, 2586, 2195, 20, 4006, 84, 23, 632, 6117, 8786, 5, 2, 1295, 1394, 6, 2, 12251, 6118, 5, 4007, 3], [45, 660, 380, 5, 10, 4, 2, 16247, 38, 2020, 26, 4008, 1595, 898, 194, 4, 9, 20, 2269, 5, 2364, 46, 329, 8787, 7847, 24, 10, 287, 7, 68, 10, 287, 5, 10, 3], [124, 151, 17, 41, 42, 539, 5, 2, 135, 230, 484, 3]]\n","train words: [[    2   350   116  7190    22    25  1127   390     7    26   157    21\n","      2   124   623     9     7    42   190     6     8  7191    77   127\n","      2   180   125   329  1111   596     3     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [   48    25    19    25    35    44  1613   243   275   998    16  3566\n","  16244     2  1476     6   353    16 16245 16246    24 10131   617     3\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [    2  2586  2195    20  4006    84    23   632  6117  8786     5     2\n","   1295  1394     6     2 12251  6118     5  4007     3     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [   45   660   380     5    10     4     2 16247    38  2020    26  4008\n","   1595   898   194     4     9    20  2269     5  2364    46   329  8787\n","   7847    24    10   287     7    68    10   287     5    10     3     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  124   151    17    41    42   539     5     2   135   230   484     3\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","before encode: [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'B-org', 'I-org', 'I-org', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'I-geo', 'O'], ['O', 'B-gpe', 'O', 'O', 'O', 'B-tim', 'O', 'B-gpe', 'B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'B-eve', 'I-eve', 'O', 'B-geo', 'O'], ['O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O']]\n","texts in encode(): ['O\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tB-gpe\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tB-geo\\tB-org\\tI-org\\tI-org\\tO', 'O\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tB-gpe\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tB-geo\\tO\\tB-geo\\tI-geo\\tO', 'O\\tB-gpe\\tO\\tO\\tO\\tB-tim\\tO\\tB-gpe\\tB-per\\tI-per\\tO\\tO\\tO\\tO\\tO\\tO\\tB-eve\\tI-eve\\tO\\tB-geo\\tO', 'O\\tO\\tO\\tO\\tB-tim\\tO\\tO\\tB-org\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tO\\tB-tim\\tO', 'B-gpe\\tO\\tO\\tO\\tO\\tB-gpe\\tO\\tO\\tO\\tB-geo\\tI-geo\\tO']\n","after encode: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 7, 7, 1], [1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 9, 1], [1, 8, 1, 1, 1, 3, 1, 8, 6, 5, 1, 1, 1, 1, 1, 1, 12, 14, 1, 2, 1], [1, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1], [8, 1, 1, 1, 1, 8, 1, 1, 1, 2, 9, 1]]\n","train labels: [[ 1  1  1  1  1  1  1  1  1  1  1  1  1  8  1  1  1  1  1  1  1  1  1  1\n","   1  1  2  4  7  7  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  1  1  1  1  1  1  1  8  1  1  1  1  1  1  1  1  1  1  2  1  2  9  1\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  8  1  1  1  3  1  8  6  5  1  1  1  1  1  1 12 14  1  2  1  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  1  1  1  3  1  1  4  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n","   1  1  1  1  1  1  1  1  1  3  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 8  1  1  1  1  8  1  1  1  2  9  1  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","loading fastText...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["filtering embeddings...\n","Epoch 1/100\n","1080/1080 [==============================] - 132s 122ms/step - loss: 0.0469 - val_loss: 0.0258\n","Epoch 2/100\n","1080/1080 [==============================] - 142s 131ms/step - loss: 0.0211 - val_loss: 0.0247\n","Epoch 3/100\n","1080/1080 [==============================] - 142s 131ms/step - loss: 0.0161 - val_loss: 0.0247\n","Epoch 4/100\n","1080/1080 [==============================] - 142s 131ms/step - loss: 0.0128 - val_loss: 0.0263\n","Epoch 5/100\n","1080/1080 [==============================] - 142s 131ms/step - loss: 0.0102 - val_loss: 0.0283\n","Epoch 6/100\n","1080/1080 [==============================] - 141s 131ms/step - loss: 0.0081 - val_loss: 0.0305\n","texts in encode(): [\"In\\tTehran\\t,\\tthe\\tchief\\tof\\tIran\\t's\\tRevolutionary\\tGuards\\t,\\tGeneral\\tYahya\\tRahim\\tSafavi\\t,\\tsaid\\tSaturday\\this\\tcountry\\twould\\tuse\\tballistic\\tmissiles\\tto\\tdefend\\titself\\tif\\tattacked\\t.\", 'Even\\tthough\\tboth\\tsites\\tare\\tnow\\tfunctioning\\t,\\tTwitter\\tsays\\tusers\\twill\\tcontinue\\tto\\texperience\\tlonger\\tload\\ttimes\\tand\\tslow\\tresponse\\t.', 'Suspected\\tU.S.\\tdrones\\thave\\tcarried\\tout\\tat\\tleast\\t0\\tmissile\\tstrikes\\ton\\tmilitant\\ttargets\\tin\\tnorthwest\\tPakistan\\tover\\tthe\\tpast\\tyear\\t.', \"President\\tBarack\\tObama\\thas\\treaffirmed\\this\\tbelief\\tin\\ta\\twoman\\t's\\tright\\tto\\tchoose\\twhether\\tto\\thave\\tan\\tabortion\\tas\\ttens\\tof\\tthousands\\tof\\tabortion\\topponents\\theld\\ttheir\\tannual\\trally\\tin\\tWashington\\t.\", \"They\\tall\\tdecided\\tthat\\tone\\tperson\\tshould\\tget\\toff\\t,\\tbecause\\tif\\tthey\\tdid\\tn't\\t,\\tthe\\trope\\twould\\tbreak\\tand\\teveryone\\twould\\tdie\\t.\"]\n","y_pred after predict: [[[3.18427442e-06 9.98642504e-01 1.90383784e-04 ... 2.05253286e-06\n","   3.30317062e-06 1.03041884e-05]\n","  [4.94561300e-06 1.17133127e-03 8.63029420e-01 ... 1.42598865e-05\n","   4.27853483e-05 7.42773591e-06]\n","  [7.28298133e-08 9.99818981e-01 2.69343423e-06 ... 1.36821043e-07\n","   3.82777259e-08 5.32571335e-07]\n","  ...\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]]\n","\n"," [[6.89271064e-06 9.98557985e-01 5.68727546e-06 ... 4.60738283e-06\n","   1.28563570e-05 1.75727582e-05]\n","  [2.51701323e-07 9.99922633e-01 9.80063191e-07 ... 1.79563500e-07\n","   7.60000432e-07 6.49462947e-07]\n","  [2.07552660e-08 9.99990582e-01 3.64137236e-07 ... 1.64896949e-08\n","   2.79750736e-07 1.48062327e-07]\n","  ...\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]]\n","\n"," [[5.26769145e-06 9.92143154e-01 4.10581939e-03 ... 3.29357936e-05\n","   1.49639982e-05 2.06052682e-05]\n","  [8.30279419e-07 1.14107982e-03 9.41028357e-01 ... 7.42469410e-06\n","   1.69247653e-06 1.22714357e-06]\n","  [9.54688744e-07 9.96554494e-01 4.47554812e-05 ... 1.30220287e-05\n","   1.56148360e-06 4.37566541e-06]\n","  ...\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]]\n","\n"," [[5.70823545e-07 1.27483904e-02 3.52565148e-05 ... 1.00515172e-05\n","   1.56837941e-05 5.48427488e-06]\n","  [2.83101349e-06 1.80865129e-04 4.49274958e-04 ... 3.40088081e-05\n","   1.34094189e-05 1.56010155e-05]\n","  [4.68254342e-07 1.27303676e-04 1.87564438e-05 ... 2.18575769e-05\n","   5.18166735e-06 1.12820144e-05]\n","  ...\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]]\n","\n"," [[2.13945964e-06 9.99429047e-01 1.99238593e-06 ... 3.21487187e-06\n","   3.57977501e-06 8.37895914e-06]\n","  [3.48066408e-07 9.99616623e-01 3.51035936e-07 ... 8.17068866e-08\n","   1.63053403e-06 1.27205863e-06]\n","  [2.96959257e-08 9.99983191e-01 5.26299061e-07 ... 2.56839394e-09\n","   1.30236245e-07 1.23381412e-07]\n","  ...\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]\n","  [5.46578094e-02 5.87577820e-02 5.72288856e-02 ... 5.49277142e-02\n","   5.53179234e-02 5.49844727e-02]]]\n","y_pred after argmax: [[1 2 1 1 1 1 2 1 2 9 1 4 7 7 7 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"," [1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"," [1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 9 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"," [6 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"," [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n","y_pred after decode: [['O', 'B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'I-geo', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n","y_pred after the last line: [['O', 'B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'I-geo', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'B-tim', 'O', 'O'], ['B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n","           precision    recall  f1-score   support\n","\n","      org     0.5901    0.6177    0.6036      3913\n","      geo     0.8337    0.8655    0.8493      7664\n","      tim     0.8708    0.8207    0.8450      4049\n","      per     0.7252    0.7164    0.7208      3389\n","      gpe     0.9442    0.9383    0.9412      3175\n","      art     0.2000    0.0233    0.0417        86\n","      eve     0.2250    0.1500    0.1800        60\n","      nat     0.8667    0.2600    0.4000        50\n","\n","micro avg     0.7935    0.7953    0.7944     22386\n","macro avg     0.7931    0.7953    0.7933     22386\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b1_w2EgHMlc8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}