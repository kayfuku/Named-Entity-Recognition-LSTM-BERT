{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_BERT_04_for_trained_model.ipynb","provenance":[{"file_id":"1ZL-ozV-8KnzTqbf8PROZHS8SBr7uRPnO","timestamp":1586231888304},{"file_id":"1lUqIXqGMWPwb96OvZZHChkvgctZjpvQP","timestamp":1585889381117},{"file_id":"1eD2syw6wZUJbsr_sbjIVyvkmTaVf8k_k","timestamp":1585886025372},{"file_id":"1HMbsNTR2ZGQ9K8D2wZINQKfefLAwDyFh","timestamp":1585796543095},{"file_id":"17QBibb081pFtgTmSnIpFV7Ixq3FzPiLH","timestamp":1579656913093}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"do805zSlwTue","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"9osmrZEE3m3Y","colab_type":"code","outputId":"73b97191-5b31-43cb-ef17-acb558c24e17","executionInfo":{"status":"ok","timestamp":1587848162641,"user_tz":420,"elapsed":293,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Mount my Google Drive. \n","from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AY7mEyiqwXw7","colab_type":"code","outputId":"c8ef2a7c-e0fd-4ccc-ba9e-0170827f0f8c","executionInfo":{"status":"ok","timestamp":1587848164600,"user_tz":420,"elapsed":2239,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["!mkdir data\n","!mkdir models"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n","mkdir: cannot create directory ‘models’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iYBoqugY37nE","colab_type":"code","colab":{}},"source":["# Copy over the trained models in Google Drive.\n","!cp -r drive/'My Drive'/cs663/bert/trained_models/model_03 ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72DpUAWU47fG","colab_type":"code","colab":{}},"source":["# Copy over the dataset in Google Drive.\n","!cp -r drive/'My Drive'/cs663/bert/test_empty_line_inserted.tsv ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BpwshrgrwBsn","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Azml1NhpYZk","colab_type":"code","outputId":"f4a86ae6-fb12-4074-b579-a94c809325ba","executionInfo":{"status":"ok","timestamp":1587848182084,"user_tz":420,"elapsed":19700,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":308}},"source":["# Type of GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sat Apr 25 20:56:11 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P0    37W / 250W |   2877MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1eh85yjpvda","colab_type":"code","outputId":"40f58b14-4c2f-4185-be9a-f5a428ee7632","executionInfo":{"status":"ok","timestamp":1587848182086,"user_tz":420,"elapsed":19694,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Memory\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3pSBNlZwGFD","colab_type":"code","outputId":"ae011c5b-061c-4234-bf4c-c91e79f563a6","executionInfo":{"status":"ok","timestamp":1587848184956,"user_tz":420,"elapsed":22556,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":563}},"source":["!pip install seqeval transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v7GSLaHewmV2","colab_type":"text"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"MVaLQ4f4wkNh","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utilities.\n","\"\"\"\n","from seqeval.metrics import classification_report, accuracy_score\n","import numpy as np\n","\n","def load_dataset(filename, encoding='utf-8'):\n","    \"\"\"Loads data and label from a file.\n","    Args:\n","        filename (str): path to the file.\n","        encoding (str): file encoding format.\n","        The file format is tab-separated values.\n","        A blank line is required at the end of a sentence.\n","        For example:\n","        ```\n","        EU\tB-ORG\n","        rejects\tO\n","        German\tB-MISC\n","        call\tO\n","        to\tO\n","        boycott\tO\n","        British\tB-MISC\n","        lamb\tO\n","        .\tO\n","        Peter\tB-PER\n","        Blackburn\tI-PER\n","        ...\n","        ```\n","    Returns:\n","        tuple(numpy array, numpy array): data and labels.\n","    Example:\n","        >>> filename = 'conll2003/en/ner/train.txt'\n","        >>> data, labels = load_data_and_labels(filename)\n","    \"\"\"\n","    sentences, labels = [], []\n","    words, tags = [], []\n","    with open(filename, encoding=encoding) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line:\n","                word, tag = line.split('\\t')\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                sentences.append(words)\n","                labels.append(tags)\n","                words, tags = [], []\n","        if words:\n","            sentences.append(words)\n","            labels.append(tags)\n","\n","    return sentences, labels\n","\n","# For BERT\n","def evaluate(model, labels_vocab, features, label_ids_true):\n","    # Predict. \n","    label_ids_pred = model.predict(features)\n","    print('label_ids_pred after predict:\\n', label_ids_pred[:5])\n","    label_ids_pred = np.argmax(label_ids_pred[0], axis=-1) # label_ids_pred[0] <= typo corrected! \n","    print('label_ids_pred after argmax:\\n', label_ids_pred[:5])\n","    print('label_ids_true:\\n', label_ids_true[:5])\n","\n","    y_pred = [[] for _ in range(label_ids_pred.shape[0])]\n","    y_true = [[] for _ in range(label_ids_pred.shape[0])]\n","    for i in range(label_ids_pred.shape[0]):\n","        for j in range(label_ids_pred.shape[1]):\n","            if label_ids_true[i][j] == 0:\n","                continue\n","            y_pred[i].append(label_ids_pred[i][j])\n","            y_true[i].append(label_ids_true[i][j])\n","\n","    y_pred = labels_vocab.decode(y_pred)\n","    print('y_pred:\\n', y_pred[:5])\n","    y_true = labels_vocab.decode(y_true)\n","    print('y_true:\\n', y_true[:5])\n","    print(classification_report(y_true, y_pred, digits=4))\n","    print('Accuracy:', accuracy_score(y_true, y_pred))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIl_MoqCwqAu","colab_type":"text"},"source":["# preprocessing.py"]},{"cell_type":"code","metadata":{"id":"wBb1bBB4wpUR","colab_type":"code","colab":{}},"source":["import re\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class Vocab:\n","\n","    def __init__(self, num_words=None, lower=True, oov_token=None):\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","            num_words=num_words, # max size of vocabulary\n","            oov_token=oov_token,\n","            filters='',\n","            lower=lower,\n","            split='\\t'\n","        )\n","\n","    def fit(self, sequences):\n","        texts = self._texts(sequences)\n","        # Create vocabulary. \n","        self.tokenizer.fit_on_texts(texts)\n","        return self\n","\n","    def encode(self, sequences):\n","        \"\"\" Convert words to ids \"\"\"\n","        texts = self._texts(sequences)\n","        # print('texts in encode():', texts[:5]) # list of strings (one string per sentence)\n","        return self.tokenizer.texts_to_sequences(texts) # For one string, change string to list of ids. \n","\n","    def decode(self, sequences):\n","        # print('sequences in decode:\\n', sequences)\n","        texts = self.tokenizer.sequences_to_texts(sequences)\n","        return [text.split(' ') for text in texts]\n","\n","    def _texts(self, sequences):\n","        return ['\\t'.join(words) for words in sequences]\n","\n","    def get_index(self, word):\n","        return self.tokenizer.word_index.get(word)\n","\n","    @property\n","    def size(self):\n","        \"\"\"Return vocabulary size.\"\"\"\n","        return len(self.tokenizer.word_index) + 1\n","\n","    def save(self, file_path):\n","        with open(file_path, 'w') as f:\n","            config = self.tokenizer.to_json()\n","            f.write(config)\n","\n","    @classmethod\n","    def load(cls, file_path):\n","        with open(file_path) as f:\n","            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n","            vocab = cls()\n","            vocab.tokenizer = tokenizer\n","        return vocab\n","\n","\n","def normalize_number(text, reduce=True):\n","    \"\"\" Replace numbers with 0. \"\"\"\n","    if reduce:\n","        normalized_text = re.sub(r'\\d+', '0', text)\n","    else:\n","        # Keep the length same. \n","        normalized_text = re.sub(r'\\d', '0', text)\n","    return normalized_text\n","\n","\n","def preprocess_dataset(sequences):\n","    sequences = [[normalize_number(w) for w in words] for words in sequences]\n","    return sequences\n","\n","\n","def create_dataset(sequences, vocab):\n","    # print('before encode:', sequences[:5])\n","    sequences = vocab.encode(sequences)\n","    # print('after encode:', sequences[:5])\n","    # Padding\n","    sequences = pad_sequences(sequences, padding='post')\n","    return sequences\n","\n","\n","# Create inputs for BERT. \n","def convert_examples_to_features(x, # features\n","                                 y, # labels\n","                                 vocab, # Vocabulary of lebels\n","                                 max_seq_length,\n","                                 tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","        'label_ids': []\n","    }\n","    for words, labels in zip(x, y):\n","        # print('words:', words) # sentence\n","        # print('labels:', labels) # labels in the sentence\n","\n","        # For each sentence \n","        tokens = [tokenizer.cls_token] # [CLS]\n","        # print('tokens:', tokens)\n","\n","        label_ids = [pad_token]\n","        for word, label in zip(words, labels):\n","            # For each word  \n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            label_id = vocab.get_index(label)\n","            label_ids.extend([label_id] + [pad_token] * (len(word_tokens) - 1))\n","\n","        tokens += [tokenizer.sep_token] # [SEP]\n","\n","        # print('tokens before convert_tokens_to_ids:\\n', tokens) \n","        # ['[CLS]', 'キ', '##ケ', '##ロ', 'は', '、', 'カエサル', 'と', 'は', ..., 'こと', 'と', 'なっ', 'た', '。', '[SEP]']\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        # print('input_ids:\\n', input_ids)\n","        # [2, 185, 28719, 28505, 9, 6, 18936, 13, ... ]\n","\n","        attention_mask = [1] * len(input_ids)\n","        # print('attention_mask:\\n', attention_mask)\n","        # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]\n","\n","        token_type_ids = [pad_token] * max_seq_length\n","        # print('token_type_ids:\\n', token_type_ids)\n","        # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]\n","\n","        # print('label_ids:\\n', label_ids)\n","        # [0, 7, 0, 0, 1, 1, 7, 1, 1, 1, 1, 16, 15, 1, 1, ... ]\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","        features['label_ids'].append(label_ids)\n","\n","    # Padding\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    y = features['label_ids']\n","    return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhcqeEKwwDg","colab_type":"text"},"source":["# models.py"]},{"cell_type":"code","metadata":{"id":"x2BAMgPuwuBU","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional\n","from transformers import TFBertForTokenClassification, BertConfig\n","\n","\n","class UnidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim,\n","                         return_sequences=True, # Point! True: Sequence Labeling\n","                         name='lstm')\n","        # output_dim: label_vocab.size()\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.lstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class BidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        lstm = LSTM(hid_dim,\n","                    return_sequences=True,\n","                    name='lstm')\n","        # Wrap the LSTM with Bidirectional. \n","        self.bilstm = Bidirectional(lstm, name='bilstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        bilstm = self.bilstm(embedding)\n","        y = self.fc(bilstm)\n","        return Model(inputs=x, outputs=y)\n","\n","# For BERT\n","def build_model(pretrained_model_name_or_path, num_labels):\n","    # BertConfig holds configuration for BERT. \n","    # Read configuration from pre-trained model. \n","    config = BertConfig.from_pretrained(\n","        pretrained_model_name_or_path,\n","        num_labels=num_labels\n","    )\n","    # BERT for sequence labelling\n","    model = TFBertForTokenClassification.from_pretrained(\n","        pretrained_model_name_or_path,\n","        config=config\n","    )\n","    # Add a Dense layer with softmax to the last layer.  \n","    model.layers[-1].activation = tf.keras.activations.softmax\n","    \n","    return model\n","\n","\n","def loss_func(num_labels):\n","    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def loss(y_true, y_pred):\n","        input_mask = tf.not_equal(y_true, 0)\n","        logits = tf.reshape(y_pred, (-1, num_labels))\n","        active_loss = tf.reshape(input_mask, (-1,))\n","        # Remove paddings. \n","        active_logits = tf.boolean_mask(logits, active_loss)\n","        train_labels = tf.reshape(y_true, (-1,))\n","        # Remove paddings. \n","        active_labels = tf.boolean_mask(train_labels, active_loss)\n","        cross_entropy = loss_fct(active_labels, active_logits)\n","        return cross_entropy\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAO2aikw99QU","colab_type":"text"},"source":["# train_bert.py\n","\n"]},{"cell_type":"code","metadata":{"id":"KsHyuz10-E2a","colab_type":"code","outputId":"ba6a1209-c07c-4e7a-b500-7c67ee802b94","executionInfo":{"status":"ok","timestamp":1587848343588,"user_tz":420,"elapsed":181166,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from transformers import BertTokenizer\n","\n","# Set hyper-parameters.\n","batch_size = 32\n","epochs = 100\n","# epochs = 20 # For debugging\n","model_path = 'models/'\n","# Pre-trained model trained on cased English text.\n","pretrained_model_from_original = 'bert-base-cased'\n","pretrained_model = 'model_03'\n","maxlen = 250\n","\n","# Data loading.\n","x, y = load_dataset('./test_empty_line_inserted.tsv')    \n","\n","# Tokenizer from BERT\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model_from_original, do_word_tokenize=False)\n","\n","# Pre-processing.\n","x = preprocess_dataset(x) # Normalize numbers.\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","labels_vocab = Vocab(lower=False).fit(y_train)\n","\n","features_train, labels_train = convert_examples_to_features(\n","    x_train,\n","    y_train,\n","    # x_train[:5], # For debugging\n","    # y_train[:5], # For debugging\n","    labels_vocab,\n","    max_seq_length=maxlen,\n","    tokenizer=tokenizer\n",")\n","\n","features_test, labels_test = convert_examples_to_features(\n","    x_test,\n","    y_test,\n","    # x_test[:5], # For debugging\n","    # y_test[:5], # For debugging \n","    labels_vocab,\n","    max_seq_length=maxlen,\n","    tokenizer=tokenizer\n",")\n","\n","# Build model from my trained model. \n","model = build_model(pretrained_model, labels_vocab.size)\n","model.compile(optimizer='sgd', loss=loss_func(labels_vocab.size))\n","\n","# Evaluation.\n","evaluate(model, labels_vocab, features_test, labels_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["label_ids_pred after predict:\n"," (array([[[3.84216546e-05, 9.91067827e-01, 1.51462213e-03, ...,\n","         3.36913508e-05, 1.93526575e-04, 8.07656397e-05],\n","        [7.76241052e-07, 9.99349892e-01, 2.89793446e-04, ...,\n","         9.14278814e-07, 5.11719009e-06, 1.04136654e-06],\n","        [8.14651648e-05, 2.78127816e-04, 9.35651302e-01, ...,\n","         4.98961126e-05, 3.91494192e-04, 8.05361196e-05],\n","        ...,\n","        [3.85773092e-06, 9.96182024e-01, 9.98642761e-04, ...,\n","         8.40593839e-06, 2.24059186e-05, 6.60109345e-06],\n","        [3.31167007e-06, 9.96991158e-01, 7.23084202e-04, ...,\n","         6.84519409e-06, 1.96578640e-05, 5.55329143e-06],\n","        [2.03866671e-06, 9.98490214e-01, 3.33804317e-04, ...,\n","         3.63085201e-06, 1.14772984e-05, 3.38397103e-06]],\n","\n","       [[1.16683383e-04, 9.75640655e-01, 2.81129475e-03, ...,\n","         8.11719001e-05, 5.65593888e-04, 2.46033160e-04],\n","        [2.44198645e-07, 9.99945521e-01, 3.63537424e-06, ...,\n","         1.53141258e-07, 9.79434503e-07, 3.52907279e-07],\n","        [2.47786801e-07, 9.99950409e-01, 3.29034719e-06, ...,\n","         1.84396072e-07, 8.26475230e-07, 3.55523582e-07],\n","        ...,\n","        [6.44988613e-05, 8.18590224e-01, 1.90281961e-02, ...,\n","         1.42567776e-04, 2.75673461e-04, 7.48549501e-05],\n","        [1.10515621e-05, 9.82569218e-01, 2.12261127e-03, ...,\n","         2.18928217e-05, 5.15475658e-05, 1.49471880e-05],\n","        [2.21769787e-05, 9.73290861e-01, 4.02118172e-03, ...,\n","         3.89608394e-05, 1.08581531e-04, 3.04768619e-05]],\n","\n","       [[1.29730921e-04, 9.74442244e-01, 2.79427157e-03, ...,\n","         1.09697903e-04, 6.53348165e-04, 2.83958623e-04],\n","        [6.59214095e-07, 9.99575078e-01, 2.81462162e-05, ...,\n","         5.25233531e-07, 2.67625637e-06, 7.52962421e-07],\n","        [6.76832769e-07, 9.99790728e-01, 1.23421096e-05, ...,\n","         9.63679554e-07, 1.41563646e-06, 7.44118665e-07],\n","        ...,\n","        [1.40873672e-04, 6.81118011e-01, 4.29728292e-02, ...,\n","         2.62247631e-04, 5.86699694e-04, 1.55453483e-04],\n","        [1.29743465e-04, 7.44684041e-01, 3.57051753e-02, ...,\n","         2.40643596e-04, 6.50766829e-04, 1.65119258e-04],\n","        [8.57527411e-05, 8.71692300e-01, 1.80812199e-02, ...,\n","         1.51685774e-04, 3.29752016e-04, 9.21858955e-05]],\n","\n","       ...,\n","\n","       [[5.34178616e-05, 9.86309052e-01, 2.08477792e-03, ...,\n","         5.11905091e-05, 2.48332915e-04, 1.27007545e-04],\n","        [9.24451251e-06, 1.34183050e-04, 6.46154731e-05, ...,\n","         1.11856956e-04, 5.87158138e-06, 8.26408541e-06],\n","        [2.28038207e-05, 9.13184322e-03, 7.67676902e-05, ...,\n","         5.34082465e-05, 4.46712256e-05, 3.36902995e-05],\n","        ...,\n","        [5.51346375e-06, 9.92637157e-01, 1.40634773e-03, ...,\n","         1.65994534e-05, 2.50666253e-05, 1.06301459e-05],\n","        [5.61132838e-05, 8.80695522e-01, 3.95224094e-02, ...,\n","         1.36268514e-04, 2.13385865e-04, 6.74507319e-05],\n","        [5.46145493e-05, 9.75389481e-01, 5.73109929e-03, ...,\n","         1.07080210e-04, 2.11515362e-04, 5.97454818e-05]],\n","\n","       [[7.21467004e-05, 9.86522734e-01, 1.93014310e-03, ...,\n","         5.28803903e-05, 3.59528873e-04, 1.35602895e-04],\n","        [2.76334913e-06, 9.53787625e-01, 3.07769544e-04, ...,\n","         2.21387154e-06, 1.12728139e-05, 3.76085814e-06],\n","        [1.23860600e-05, 4.31919325e-04, 4.18335106e-03, ...,\n","         3.75172349e-05, 3.83834558e-05, 2.29468842e-05],\n","        ...,\n","        [1.31003500e-04, 6.47523766e-03, 1.04567653e-03, ...,\n","         1.27219231e-04, 2.99558742e-04, 2.78536725e-04],\n","        [5.69281518e-04, 3.01005989e-02, 4.24885303e-02, ...,\n","         4.90589184e-04, 2.04185280e-03, 6.31881412e-04],\n","        [2.70223740e-04, 3.01907491e-03, 5.87955175e-04, ...,\n","         2.27772456e-04, 2.45679490e-04, 4.77422174e-04]],\n","\n","       [[1.36043454e-04, 9.66960430e-01, 5.79888839e-03, ...,\n","         1.09162720e-04, 6.54438743e-04, 3.16833728e-04],\n","        [8.92694516e-06, 2.87034236e-05, 1.94441222e-04, ...,\n","         8.74163234e-05, 6.61549484e-06, 7.40443238e-06],\n","        [3.57448334e-06, 9.95302796e-01, 1.38189891e-04, ...,\n","         1.05359004e-05, 7.37534083e-06, 7.12668361e-06],\n","        ...,\n","        [3.17238475e-04, 4.78489429e-01, 9.06890035e-02, ...,\n","         7.47819082e-04, 9.96238086e-04, 3.70371359e-04],\n","        [1.90400504e-04, 6.06885135e-01, 6.42772019e-02, ...,\n","         4.68533748e-04, 8.70975782e-04, 2.27214870e-04],\n","        [3.08294693e-04, 3.71929914e-01, 1.30148798e-01, ...,\n","         7.06868886e-04, 9.94500471e-04, 3.69185262e-04]]], dtype=float32),)\n","label_ids_pred after argmax:\n"," [[1 1 2 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]\n"," [1 6 5 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]]\n","label_ids_true:\n"," [[0 1 3 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]\n"," [0 6 5 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]]\n","y_pred:\n"," [['O', 'B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-org', 'I-geo', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'B-tim', 'O', 'O'], ['B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n","y_true:\n"," [['O', 'B-tim', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'I-geo', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'B-tim', 'O', 'O'], ['B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n","           precision    recall  f1-score   support\n","\n","      org     0.7002    0.6619    0.6805      3913\n","      per     0.7155    0.8014    0.7560      3389\n","      tim     0.8039    0.8614    0.8317      4049\n","      gpe     0.8297    0.9528    0.8870      3175\n","      geo     0.8439    0.8847    0.8638      7664\n","      eve     0.1935    0.2000    0.1967        60\n","      nat     0.3333    0.1400    0.1972        50\n","      art     0.1316    0.0581    0.0806        86\n","\n","micro avg     0.7879    0.8319    0.8093     22386\n","macro avg     0.7845    0.8319    0.8066     22386\n","\n","Accuracy: 0.967358269318209\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H5uLHFq_xYjg","colab_type":"code","outputId":"a89e393b-b9be-445d-a2a4-2b064e81b2a9","executionInfo":{"status":"ok","timestamp":1587848343589,"user_tz":420,"elapsed":181156,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"tf_bert_for_token_classification_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  108310272 \n","_________________________________________________________________\n","dropout_75 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  13842     \n","=================================================================\n","Total params: 108,324,114\n","Trainable params: 108,324,114\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AdacvBSPSq1x","colab_type":"text"},"source":["# Apply the model to a new sentence"]},{"cell_type":"code","metadata":{"id":"tFOj2st0wWKb","colab_type":"code","colab":{}},"source":["# Create inputs for BERT. \n","def create_inputs(x, # features\n","                  vocab, # Vocabulary of lebels\n","                  max_seq_length,\n","                  tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","    }\n","    for words in x:\n","        print('words:', words) # sentence\n","\n","        # For each sentence \n","        tokens = [tokenizer.cls_token] # [CLS]\n","        print('tokens:', tokens)\n","\n","        for word in words:\n","            # For each word  \n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","\n","        tokens += [tokenizer.sep_token] # [SEP]\n","\n","        print('tokens before convert_tokens_to_ids:\\n', tokens) \n","        # ['[CLS]', 'キ', '##ケ', '##ロ', 'は', '、', 'カエサル', 'と', 'は', ..., 'こと', 'と', 'なっ', 'た', '。', '[SEP]']\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        print('input_ids:\\n', input_ids)\n","        # [2, 185, 28719, 28505, 9, 6, 18936, 13, ... ]\n","\n","        attention_mask = [1] * len(input_ids)\n","        print('attention_mask:\\n', attention_mask)\n","        # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]\n","\n","        token_type_ids = [pad_token] * max_seq_length\n","        print('token_type_ids:\\n', token_type_ids)\n","        # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","\n","    # Padding\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    return x, tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYNna5esy7av","colab_type":"code","outputId":"95ff9f05-2e97-40f7-fdd2-1784cb406785","executionInfo":{"status":"ok","timestamp":1587848343590,"user_tz":420,"elapsed":181146,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# test_sentence = \"\"\"\n","# Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \n","# reporter for the network, about protests in Minnesota and elsewhere. \n","# \"\"\"\n","\n","# test_sentence = \"\"\"\n","# The problems mainly happen with rapid tests,” said Dr. Giorgio Palù, an Italian microbiologist \n","# and former president of the European Society for Virology. “They will never be able to tell \n","# the spread of the virus because they do not have the required sensitivity and specificity. \n","# \"\"\"\n","\n","# test_sentence = \"\"\"\n","# This month, the F.D.A. warned that some firms marketing their antibody tests in the United States \n","# were falsely claiming that they had formal federal approval, or that they could diagnose Covid-19. \n","# \"\"\"\n","\n","test_sentence = \"\"\"\n","Jim bought 300 shares of Acme Corp. in 2006. \n","\"\"\"\n","\n","\n","test_sentence = [test_sentence.split()]\n","test_sentence"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Jim', 'bought', '300', 'shares', 'of', 'Acme', 'Corp.', 'in', '2006.']]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"P1eMPBvF4coZ","colab_type":"code","outputId":"70b8c542-f811-4bf3-9ef1-d5b46dd043fe","executionInfo":{"status":"ok","timestamp":1587848343592,"user_tz":420,"elapsed":181142,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["x, tokens = create_inputs(test_sentence, labels_vocab, maxlen, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["words: ['Jim', 'bought', '300', 'shares', 'of', 'Acme', 'Corp.', 'in', '2006.']\n","tokens: ['[CLS]']\n","tokens before convert_tokens_to_ids:\n"," ['[CLS]', 'Jim', 'bought', '300', 'shares', 'of', 'A', '##c', '##me', 'Corp', '.', 'in', '2006', '.', '[SEP]']\n","input_ids:\n"," [101, 3104, 3306, 3127, 6117, 1104, 138, 1665, 3263, 13619, 119, 1107, 1386, 119, 102]\n","attention_mask:\n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","token_type_ids:\n"," [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xklrbr5MySMD","colab_type":"code","outputId":"fcc3c75c-b7ab-49d9-c73f-d6f5f964bb27","executionInfo":{"status":"ok","timestamp":1587848343707,"user_tz":420,"elapsed":181250,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["# Predict. \n","label_ids_pred = model.predict(x)\n","print('label_ids_pred after predict:\\n', label_ids_pred)\n","label_ids_pred = np.argmax(label_ids_pred[0], axis=-1) # label_ids_pred[0] <= typo corrected! \n","print('label_ids_pred after argmax:\\n', label_ids_pred)\n","\n","y_pred = [[] for _ in range(label_ids_pred.shape[0])]\n","# y_true = [[] for _ in range(label_ids_pred.shape[0])]\n","for i in range(label_ids_pred.shape[0]):\n","    for j in range(len(tokens)):\n","    # for j in range(label_ids_pred.shape[1]):\n","        # if label_ids_true[i][j] == 0:\n","        #     continue\n","        y_pred[i].append(label_ids_pred[i][j])\n","        # y_true[i].append(label_ids_true[i][j])\n","\n","y_pred = labels_vocab.decode(y_pred)\n","print('y_pred:\\n', y_pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["label_ids_pred after predict:\n"," (array([[[3.4810786e-04, 9.2185426e-01, 1.0244295e-02, ...,\n","         2.6739223e-04, 1.2339789e-03, 6.3977827e-04],\n","        [1.0570589e-04, 1.7173028e-03, 2.9354591e-03, ...,\n","         5.5777517e-05, 2.7726963e-04, 9.0894187e-05],\n","        [2.3801859e-07, 9.9995172e-01, 2.2949707e-06, ...,\n","         2.5984363e-07, 5.5892161e-07, 3.7659345e-07],\n","        ...,\n","        [4.1827859e-04, 4.4166663e-01, 6.7779571e-02, ...,\n","         6.0617918e-04, 1.0953184e-03, 4.5898106e-04],\n","        [1.3111862e-04, 8.2915807e-01, 2.0256037e-02, ...,\n","         2.4486790e-04, 4.2682173e-04, 1.7149434e-04],\n","        [3.2199372e-04, 5.9775680e-01, 5.9321303e-02, ...,\n","         5.0845358e-04, 8.5004308e-04, 3.4872879e-04]]], dtype=float32),)\n","label_ids_pred after argmax:\n"," [[1 6 1 1 1 1 4 7 7 7 7 1 3 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1\n","  1 1 4 7 7 7 7 3 1 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1 1 1 4 1\n","  4 7 1 7 1 1 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1 1 1 4 4 7 7 7\n","  7 3 3 1 1 3 3 1 1 1 1 1 1 1 4 4 7 7 7 7 7 7 7 7 3 3 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 4 4 7 7 7 4 7 7 7 3 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 4 4 4 7\n","  7 7 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 4 4 7 7 7 7 3 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 7 7 7 7 1 1 1 7 1 1 1 1 1 1 1 1 1]]\n","y_pred:\n"," [['O', 'B-per', 'O', 'O', 'O', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'I-org', 'O', 'B-tim', 'O', 'O']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWkclhhKzpIy","colab_type":"code","colab":{}},"source":["#  ['[CLS]', 'Mr', '.', 'Trump', '’', 's', 't', '##weet', '##s', 'began', 'just', 'moments', 'after', 'a', 'Fox', 'News', 'report', 'by', 'Mike', 'To', '##bin', ',', 'a', 'reporter', 'for', 'the', 'network', ',', 'about', 'protests', 'in', 'Minnesota', 'and', 'elsewhere', '.', '[SEP]']\n","\n","new_tokens, new_labels = [], []\n","for token, label in zip(tokens, y_pred[0]):\n","  if token.startswith(\"##\"): \n","    # Concatenate the word pieces to one word.    \n","    new_tokens[-1] = new_tokens[-1] + token[2:]\n","  else:\n","    new_labels.append(label)\n","    new_tokens.append(token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCRDYomq9O-y","colab_type":"code","outputId":"5fdd4067-a57d-4c13-b67b-6e0bbfae14fd","executionInfo":{"status":"ok","timestamp":1587848343709,"user_tz":420,"elapsed":181241,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["for token, label in zip(new_tokens, new_labels):\n","    print(\"{}\\t\\t\\t{}\".format(token, label))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[CLS]\t\t\tO\n","Jim\t\t\tB-per\n","bought\t\t\tO\n","300\t\t\tO\n","shares\t\t\tO\n","of\t\t\tO\n","Acme\t\t\tB-org\n","Corp\t\t\tI-org\n",".\t\t\tI-org\n","in\t\t\tO\n","2006\t\t\tB-tim\n",".\t\t\tO\n","[SEP]\t\t\tO\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AZsTFtN5-VRu","colab_type":"code","colab":{}},"source":["###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FXYhWXKLOz5","colab_type":"code","colab":{}},"source":["###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71bfyEX5L6QM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}