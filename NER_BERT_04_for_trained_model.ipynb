{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_BERT_04_for_trained_model.ipynb","provenance":[{"file_id":"1ZL-ozV-8KnzTqbf8PROZHS8SBr7uRPnO","timestamp":1586231888304},{"file_id":"1lUqIXqGMWPwb96OvZZHChkvgctZjpvQP","timestamp":1585889381117},{"file_id":"1eD2syw6wZUJbsr_sbjIVyvkmTaVf8k_k","timestamp":1585886025372},{"file_id":"1HMbsNTR2ZGQ9K8D2wZINQKfefLAwDyFh","timestamp":1585796543095},{"file_id":"17QBibb081pFtgTmSnIpFV7Ixq3FzPiLH","timestamp":1579656913093}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3b8d3145bd0c4d19999a6c7a5f3cd8ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2f4ef80860984f85b42b32597a36ae9f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a7b06fd583f740efb0e3cc3a970485aa","IPY_MODEL_02f866cf7adc481bb1b66fd0db2fb9a9"]}},"2f4ef80860984f85b42b32597a36ae9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7b06fd583f740efb0e3cc3a970485aa":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8e9bc76ae2b04253baed2d0599064b87","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b5e316b0fa44570b62616baf426bac9"}},"02f866cf7adc481bb1b66fd0db2fb9a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b899817261114557803141203bc73255","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 901kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ede0591fdb740acbd25db169e631807"}},"8e9bc76ae2b04253baed2d0599064b87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6b5e316b0fa44570b62616baf426bac9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b899817261114557803141203bc73255":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ede0591fdb740acbd25db169e631807":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"do805zSlwTue","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"9osmrZEE3m3Y","colab_type":"code","outputId":"d7b41d06-d3d9-493c-cd01-0ed5599ec4f6","executionInfo":{"status":"ok","timestamp":1587532427992,"user_tz":420,"elapsed":18056,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["# Mount my Google Drive. \n","from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AY7mEyiqwXw7","colab_type":"code","colab":{}},"source":["!mkdir data\n","!mkdir models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYBoqugY37nE","colab_type":"code","colab":{}},"source":["# Copy over the trained models in Google Drive.\n","!cp -r drive/'My Drive'/cs663/bert/trained_models/model_03 ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72DpUAWU47fG","colab_type":"code","colab":{}},"source":["# Copy over the dataset in Google Drive.\n","!cp -r drive/'My Drive'/cs663/bert/test_empty_line_inserted.tsv ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BpwshrgrwBsn","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Azml1NhpYZk","colab_type":"code","outputId":"0ada83c6-3185-49cd-b185-b2575fe4e08a","executionInfo":{"status":"ok","timestamp":1587532437772,"user_tz":420,"elapsed":27819,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# Type of GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wed Apr 22 05:13:56 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1eh85yjpvda","colab_type":"code","outputId":"cbe7e4c1-22fb-4087-9282-b0d6e467cbd4","executionInfo":{"status":"ok","timestamp":1587532437984,"user_tz":420,"elapsed":28025,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Memory\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3pSBNlZwGFD","colab_type":"code","outputId":"1b196ad3-8a0d-40ed-8524-9465f9d94daf","executionInfo":{"status":"ok","timestamp":1587532448345,"user_tz":420,"elapsed":38380,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":928}},"source":["!pip install seqeval transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.2)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 22.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.0MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 49.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=57db8a5a7fe53bf84993af88c4f9756ebb5da851d87dc1d62879ba42a79cec76\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=fe09f710d4541d83fe5bdca28ddcbfc0d7346a835505faf594c891a65b0f8ff0\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 seqeval-0.0.12 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v7GSLaHewmV2","colab_type":"text"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"MVaLQ4f4wkNh","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utilities.\n","\"\"\"\n","from seqeval.metrics import classification_report, accuracy_score\n","import numpy as np\n","\n","def load_dataset(filename, encoding='utf-8'):\n","    \"\"\"Loads data and label from a file.\n","    Args:\n","        filename (str): path to the file.\n","        encoding (str): file encoding format.\n","        The file format is tab-separated values.\n","        A blank line is required at the end of a sentence.\n","        For example:\n","        ```\n","        EU\tB-ORG\n","        rejects\tO\n","        German\tB-MISC\n","        call\tO\n","        to\tO\n","        boycott\tO\n","        British\tB-MISC\n","        lamb\tO\n","        .\tO\n","        Peter\tB-PER\n","        Blackburn\tI-PER\n","        ...\n","        ```\n","    Returns:\n","        tuple(numpy array, numpy array): data and labels.\n","    Example:\n","        >>> filename = 'conll2003/en/ner/train.txt'\n","        >>> data, labels = load_data_and_labels(filename)\n","    \"\"\"\n","    sentences, labels = [], []\n","    words, tags = [], []\n","    with open(filename, encoding=encoding) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line:\n","                word, tag = line.split('\\t')\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                sentences.append(words)\n","                labels.append(tags)\n","                words, tags = [], []\n","        if words:\n","            sentences.append(words)\n","            labels.append(tags)\n","\n","    return sentences, labels\n","\n","# For BERT\n","def evaluate(model, labels_vocab, features, label_ids_true):\n","    # Predict. \n","    label_ids_pred = model.predict(features)\n","    print('label_ids_pred after predict:\\n', label_ids_pred)\n","    label_ids_pred = np.argmax(label_ids_pred[0], axis=-1) # label_ids_pred[0] <= typo corrected! \n","    print('label_ids_pred after argmax:\\n', label_ids_pred)\n","    print('label_ids_true:\\n', label_ids_true)\n","\n","    y_pred = [[] for _ in range(label_ids_pred.shape[0])]\n","    y_true = [[] for _ in range(label_ids_pred.shape[0])]\n","    for i in range(label_ids_pred.shape[0]):\n","        for j in range(label_ids_pred.shape[1]):\n","            if label_ids_true[i][j] == 0:\n","                continue\n","            y_pred[i].append(label_ids_pred[i][j])\n","            y_true[i].append(label_ids_true[i][j])\n","\n","    y_pred = labels_vocab.decode(y_pred)\n","    print('y_pred:\\n', y_pred)\n","    y_true = labels_vocab.decode(y_true)\n","    print('y_true:\\n', y_true)\n","    print(classification_report(y_true, y_pred, digits=4))\n","    print('Accuracy:', accuracy_score(y_true, y_pred))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIl_MoqCwqAu","colab_type":"text"},"source":["# preprocessing.py"]},{"cell_type":"code","metadata":{"id":"wBb1bBB4wpUR","colab_type":"code","colab":{}},"source":["import re\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","class Vocab:\n","\n","    def __init__(self, num_words=None, lower=True, oov_token=None):\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","            num_words=num_words, # max size of vocabulary\n","            oov_token=oov_token,\n","            filters='',\n","            lower=lower,\n","            split='\\t'\n","        )\n","\n","    def fit(self, sequences):\n","        texts = self._texts(sequences)\n","        # Create vocabulary. \n","        self.tokenizer.fit_on_texts(texts)\n","        return self\n","\n","    def encode(self, sequences):\n","        \"\"\" Convert words to ids \"\"\"\n","        texts = self._texts(sequences)\n","        # print('texts in encode():', texts[:5]) # list of strings (one string per sentence)\n","        return self.tokenizer.texts_to_sequences(texts) # For one string, change string to list of ids. \n","\n","    def decode(self, sequences):\n","        # print('sequences in decode:\\n', sequences)\n","        texts = self.tokenizer.sequences_to_texts(sequences)\n","        return [text.split(' ') for text in texts]\n","\n","    def _texts(self, sequences):\n","        return ['\\t'.join(words) for words in sequences]\n","\n","    def get_index(self, word):\n","        return self.tokenizer.word_index.get(word)\n","\n","    @property\n","    def size(self):\n","        \"\"\"Return vocabulary size.\"\"\"\n","        return len(self.tokenizer.word_index) + 1\n","\n","    def save(self, file_path):\n","        with open(file_path, 'w') as f:\n","            config = self.tokenizer.to_json()\n","            f.write(config)\n","\n","    @classmethod\n","    def load(cls, file_path):\n","        with open(file_path) as f:\n","            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n","            vocab = cls()\n","            vocab.tokenizer = tokenizer\n","        return vocab\n","\n","\n","def normalize_number(text, reduce=True):\n","    \"\"\" Replace numbers with 0. \"\"\"\n","    if reduce:\n","        normalized_text = re.sub(r'\\d+', '0', text)\n","    else:\n","        # Keep the length same. \n","        normalized_text = re.sub(r'\\d', '0', text)\n","    return normalized_text\n","\n","\n","def preprocess_dataset(sequences):\n","    sequences = [[normalize_number(w) for w in words] for words in sequences]\n","    return sequences\n","\n","\n","def create_dataset(sequences, vocab):\n","    # print('before encode:', sequences[:5])\n","    sequences = vocab.encode(sequences)\n","    # print('after encode:', sequences[:5])\n","    # Padding\n","    sequences = pad_sequences(sequences, padding='post')\n","    return sequences\n","\n","\n","# Create inputs for BERT. \n","def convert_examples_to_features(x, # features\n","                                 y, # labels\n","                                 vocab, # Vocabulary of lebels\n","                                 max_seq_length,\n","                                 tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","        'label_ids': []\n","    }\n","    for words, labels in zip(x, y):\n","        # print('words:', words) # sentence\n","        # print('labels:', labels) # labels in the sentence\n","\n","        # For each sentence \n","        tokens = [tokenizer.cls_token] # [CLS]\n","        # print('tokens:', tokens)\n","\n","        label_ids = [pad_token]\n","        for word, label in zip(words, labels):\n","            # For each word  \n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            label_id = vocab.get_index(label)\n","            label_ids.extend([label_id] + [pad_token] * (len(word_tokens) - 1))\n","\n","        tokens += [tokenizer.sep_token] # [SEP]\n","\n","        # print('tokens before convert_tokens_to_ids:\\n', tokens) \n","        # ['[CLS]', 'キ', '##ケ', '##ロ', 'は', '、', 'カエサル', 'と', 'は', ..., 'こと', 'と', 'なっ', 'た', '。', '[SEP]']\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        # print('input_ids:\\n', input_ids)\n","        # [2, 185, 28719, 28505, 9, 6, 18936, 13, ... ]\n","\n","        attention_mask = [1] * len(input_ids)\n","        # print('attention_mask:\\n', attention_mask)\n","        # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]\n","\n","        token_type_ids = [pad_token] * max_seq_length\n","        # print('token_type_ids:\\n', token_type_ids)\n","        # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]\n","\n","        # print('label_ids:\\n', label_ids)\n","        # [0, 7, 0, 0, 1, 1, 7, 1, 1, 1, 1, 16, 15, 1, 1, ... ]\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","        features['label_ids'].append(label_ids)\n","\n","    # Padding\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    y = features['label_ids']\n","    return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhcqeEKwwDg","colab_type":"text"},"source":["# models.py"]},{"cell_type":"code","metadata":{"id":"x2BAMgPuwuBU","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional\n","from transformers import TFBertForTokenClassification, BertConfig\n","\n","\n","class UnidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim,\n","                         return_sequences=True, # Point! True: Sequence Labeling\n","                         name='lstm')\n","        # output_dim: label_vocab.size()\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.lstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class BidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        lstm = LSTM(hid_dim,\n","                    return_sequences=True,\n","                    name='lstm')\n","        # Wrap the LSTM with Bidirectional. \n","        self.bilstm = Bidirectional(lstm, name='bilstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        bilstm = self.bilstm(embedding)\n","        y = self.fc(bilstm)\n","        return Model(inputs=x, outputs=y)\n","\n","# For BERT\n","def build_model(pretrained_model_name_or_path, num_labels):\n","    # BertConfig holds configuration for BERT. \n","    # Read configuration from pre-trained model. \n","    config = BertConfig.from_pretrained(\n","        pretrained_model_name_or_path,\n","        num_labels=num_labels\n","    )\n","    # BERT for sequence labelling\n","    model = TFBertForTokenClassification.from_pretrained(\n","        pretrained_model_name_or_path,\n","        config=config\n","    )\n","    # Add a Dense layer with softmax to the last layer.  \n","    model.layers[-1].activation = tf.keras.activations.softmax\n","    \n","    return model\n","\n","\n","def loss_func(num_labels):\n","    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def loss(y_true, y_pred):\n","        input_mask = tf.not_equal(y_true, 0)\n","        logits = tf.reshape(y_pred, (-1, num_labels))\n","        active_loss = tf.reshape(input_mask, (-1,))\n","        # Remove paddings. \n","        active_logits = tf.boolean_mask(logits, active_loss)\n","        train_labels = tf.reshape(y_true, (-1,))\n","        # Remove paddings. \n","        active_labels = tf.boolean_mask(train_labels, active_loss)\n","        cross_entropy = loss_fct(active_labels, active_logits)\n","        return cross_entropy\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAO2aikw99QU","colab_type":"text"},"source":["# train_bert.py\n","\n"]},{"cell_type":"code","metadata":{"id":"KsHyuz10-E2a","colab_type":"code","outputId":"5c76ccc4-c1f3-46d2-c2ce-e1435e2d0388","executionInfo":{"status":"ok","timestamp":1587532631493,"user_tz":420,"elapsed":221513,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3b8d3145bd0c4d19999a6c7a5f3cd8ad","2f4ef80860984f85b42b32597a36ae9f","a7b06fd583f740efb0e3cc3a970485aa","02f866cf7adc481bb1b66fd0db2fb9a9","8e9bc76ae2b04253baed2d0599064b87","6b5e316b0fa44570b62616baf426bac9","b899817261114557803141203bc73255","2ede0591fdb740acbd25db169e631807"]}},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from transformers import BertTokenizer\n","\n","# Set hyper-parameters.\n","batch_size = 32\n","epochs = 100\n","# epochs = 20 # For debugging\n","model_path = 'models/'\n","# Pre-trained model trained on cased English text.\n","pretrained_model_from_original = 'bert-base-cased'\n","pretrained_model = 'model_03'\n","maxlen = 250\n","\n","# Data loading.\n","x, y = load_dataset('./test_empty_line_inserted.tsv')    \n","\n","# Tokenizer from BERT\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model_from_original, do_word_tokenize=False)\n","\n","# Pre-processing.\n","x = preprocess_dataset(x) # Normalize numbers.\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","labels_vocab = Vocab(lower=False).fit(y_train)\n","\n","features_train, labels_train = convert_examples_to_features(\n","    x_train,\n","    y_train,\n","    # x_train[:5], # For debugging\n","    # y_train[:5], # For debugging\n","    labels_vocab,\n","    max_seq_length=maxlen,\n","    tokenizer=tokenizer\n",")\n","\n","features_test, labels_test = convert_examples_to_features(\n","    x_test,\n","    y_test,\n","    # x_test[:5], # For debugging\n","    # y_test[:5], # For debugging \n","    labels_vocab,\n","    max_seq_length=maxlen,\n","    tokenizer=tokenizer\n",")\n","\n","# Build model from my trained model. \n","model = build_model(pretrained_model, labels_vocab.size)\n","model.compile(optimizer='sgd', loss=loss_func(labels_vocab.size))\n","\n","# Evaluation.\n","evaluate(model, labels_vocab, features_test, labels_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5uLHFq_xYjg","colab_type":"code","outputId":"f96890f2-e113-40dd-dfc3-e47f814c17a6","executionInfo":{"status":"ok","timestamp":1587532631494,"user_tz":420,"elapsed":221502,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"tf_bert_for_token_classification\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  108310272 \n","_________________________________________________________________\n","dropout_37 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  13842     \n","=================================================================\n","Total params: 108,324,114\n","Trainable params: 108,324,114\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AdacvBSPSq1x","colab_type":"text"},"source":["# Apply the model to a new sentence"]},{"cell_type":"code","metadata":{"id":"tFOj2st0wWKb","colab_type":"code","colab":{}},"source":["# Create inputs for BERT. \n","def create_inputs(x, # features\n","                  vocab, # Vocabulary of lebels\n","                  max_seq_length,\n","                  tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","    }\n","    for words in x:\n","        print('words:', words) # sentence\n","\n","        # For each sentence \n","        tokens = [tokenizer.cls_token] # [CLS]\n","        print('tokens:', tokens)\n","\n","        for word in words:\n","            # For each word  \n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","\n","        tokens += [tokenizer.sep_token] # [SEP]\n","\n","        print('tokens before convert_tokens_to_ids:\\n', tokens) \n","        # ['[CLS]', 'キ', '##ケ', '##ロ', 'は', '、', 'カエサル', 'と', 'は', ..., 'こと', 'と', 'なっ', 'た', '。', '[SEP]']\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        print('input_ids:\\n', input_ids)\n","        # [2, 185, 28719, 28505, 9, 6, 18936, 13, ... ]\n","\n","        attention_mask = [1] * len(input_ids)\n","        print('attention_mask:\\n', attention_mask)\n","        # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]\n","\n","        token_type_ids = [pad_token] * max_seq_length\n","        print('token_type_ids:\\n', token_type_ids)\n","        # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","\n","    # Padding\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    return x, tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYNna5esy7av","colab_type":"code","outputId":"c21124c7-5bb0-4cf1-c11b-a7606576e5e2","executionInfo":{"status":"ok","timestamp":1587532631495,"user_tz":420,"elapsed":221494,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# test_sentence = \"\"\"\n","# Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \n","# reporter for the network, about protests in Minnesota and elsewhere. \n","# \"\"\"\n","\n","# test_sentence = \"\"\"\n","# The problems mainly happen with rapid tests,” said Dr. Giorgio Palù, an Italian microbiologist \n","# and former president of the European Society for Virology. “They will never be able to tell \n","# the spread of the virus because they do not have the required sensitivity and specificity. \n","# \"\"\"\n","\n","# test_sentence = \"\"\"\n","# This month, the F.D.A. warned that some firms marketing their antibody tests in the United States \n","# were falsely claiming that they had formal federal approval, or that they could diagnose Covid-19. \n","# \"\"\"\n","\n","test_sentence = \"\"\"\n","Jim bought 300 shares of Acme Corp. in 2006. \n","\"\"\"\n","\n","\n","test_sentence = [test_sentence.split()]\n","test_sentence"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Jim', 'bought', '300', 'shares', 'of', 'Acme', 'Corp.', 'in', '2006.']]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"P1eMPBvF4coZ","colab_type":"code","outputId":"6521ce39-01c3-49aa-d020-9048d034ea9d","executionInfo":{"status":"ok","timestamp":1587532631495,"user_tz":420,"elapsed":221488,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["x, tokens = create_inputs(test_sentence, labels_vocab, maxlen, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["words: ['Jim', 'bought', '300', 'shares', 'of', 'Acme', 'Corp.', 'in', '2006.']\n","tokens: ['[CLS]']\n","tokens before convert_tokens_to_ids:\n"," ['[CLS]', 'Jim', 'bought', '300', 'shares', 'of', 'A', '##c', '##me', 'Corp', '.', 'in', '2006', '.', '[SEP]']\n","input_ids:\n"," [101, 3104, 3306, 3127, 6117, 1104, 138, 1665, 3263, 13619, 119, 1107, 1386, 119, 102]\n","attention_mask:\n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","token_type_ids:\n"," [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xklrbr5MySMD","colab_type":"code","outputId":"2c0b687f-3fd3-4dd7-a39e-2fcdbc6c6c99","executionInfo":{"status":"ok","timestamp":1587532631496,"user_tz":420,"elapsed":221482,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["# Predict. \n","label_ids_pred = model.predict(x)\n","print('label_ids_pred after predict:\\n', label_ids_pred)\n","label_ids_pred = np.argmax(label_ids_pred[0], axis=-1) # label_ids_pred[0] <= typo corrected! \n","print('label_ids_pred after argmax:\\n', label_ids_pred)\n","\n","y_pred = [[] for _ in range(label_ids_pred.shape[0])]\n","# y_true = [[] for _ in range(label_ids_pred.shape[0])]\n","for i in range(label_ids_pred.shape[0]):\n","    for j in range(len(tokens)):\n","    # for j in range(label_ids_pred.shape[1]):\n","        # if label_ids_true[i][j] == 0:\n","        #     continue\n","        y_pred[i].append(label_ids_pred[i][j])\n","        # y_true[i].append(label_ids_true[i][j])\n","\n","y_pred = labels_vocab.decode(y_pred)\n","print('y_pred:\\n', y_pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["label_ids_pred after predict:\n"," (array([[[3.4810786e-04, 9.2185426e-01, 1.0244295e-02, ...,\n","         2.6739223e-04, 1.2339789e-03, 6.3977827e-04],\n","        [1.0570589e-04, 1.7173028e-03, 2.9354591e-03, ...,\n","         5.5777517e-05, 2.7726963e-04, 9.0894187e-05],\n","        [2.3801859e-07, 9.9995172e-01, 2.2949707e-06, ...,\n","         2.5984363e-07, 5.5892161e-07, 3.7659345e-07],\n","        ...,\n","        [4.1827859e-04, 4.4166663e-01, 6.7779571e-02, ...,\n","         6.0617918e-04, 1.0953184e-03, 4.5898106e-04],\n","        [1.3111862e-04, 8.2915807e-01, 2.0256037e-02, ...,\n","         2.4486790e-04, 4.2682173e-04, 1.7149434e-04],\n","        [3.2199372e-04, 5.9775680e-01, 5.9321303e-02, ...,\n","         5.0845358e-04, 8.5004308e-04, 3.4872879e-04]]], dtype=float32),)\n","label_ids_pred after argmax:\n"," [[1 6 1 1 1 1 4 7 7 7 7 1 3 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1\n","  1 1 4 7 7 7 7 3 1 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1 1 1 4 1\n","  4 7 1 7 1 1 1 1 1 1 1 1 1 1 1 4 4 7 7 7 3 3 1 1 1 1 1 1 1 1 1 4 4 7 7 7\n","  7 3 3 1 1 3 3 1 1 1 1 1 1 1 4 4 7 7 7 7 7 7 7 7 3 3 1 1 1 1 1 1 1 1 1 1\n","  1 1 1 4 4 7 7 7 4 7 7 7 3 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 4 4 4 7\n","  7 7 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 4 4 7 7 7 7 3 1 1 1 1 1 1 1\n","  1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 7 7 7 7 1 1 1 7 1 1 1 1 1 1 1 1 1]]\n","y_pred:\n"," [['O', 'B-per', 'O', 'O', 'O', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'I-org', 'O', 'B-tim', 'O', 'O']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWkclhhKzpIy","colab_type":"code","colab":{}},"source":["#  ['[CLS]', 'Mr', '.', 'Trump', '’', 's', 't', '##weet', '##s', 'began', 'just', 'moments', 'after', 'a', 'Fox', 'News', 'report', 'by', 'Mike', 'To', '##bin', ',', 'a', 'reporter', 'for', 'the', 'network', ',', 'about', 'protests', 'in', 'Minnesota', 'and', 'elsewhere', '.', '[SEP]']\n","\n","new_tokens, new_labels = [], []\n","for token, label in zip(tokens, y_pred[0]):\n","  if token.startswith(\"##\"): \n","    # Concatenate the word pieces to one word.    \n","    new_tokens[-1] = new_tokens[-1] + token[2:]\n","  else:\n","    new_labels.append(label)\n","    new_tokens.append(token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCRDYomq9O-y","colab_type":"code","outputId":"51b64112-c221-42ed-8907-99ce2589e2f8","executionInfo":{"status":"ok","timestamp":1587532631496,"user_tz":420,"elapsed":221471,"user":{"displayName":"Kei Fukutani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpnJD63Ph-KV2LFjvMsRnVtb8Hzv-DVjmy_kju=s64","userId":"07242443700067686241"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["for token, label in zip(new_tokens, new_labels):\n","    print(\"{}\\t\\t\\t{}\".format(token, label))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[CLS]\t\t\tO\n","Jim\t\t\tB-per\n","bought\t\t\tO\n","300\t\t\tO\n","shares\t\t\tO\n","of\t\t\tO\n","Acme\t\t\tB-org\n","Corp\t\t\tI-org\n",".\t\t\tI-org\n","in\t\t\tO\n","2006\t\t\tB-tim\n",".\t\t\tO\n","[SEP]\t\t\tO\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AZsTFtN5-VRu","colab_type":"code","colab":{}},"source":["###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FXYhWXKLOz5","colab_type":"code","colab":{}},"source":["###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71bfyEX5L6QM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}